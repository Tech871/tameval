[meta]
task = "update"
scenario = "update_test"

[lang_info]
lang = "Python"
python_version = "3.11"
python_cfg_file = "requirements.txt"

[repo_info]
repository = "google/langfun"
sha = "11d646e44baaea09bb792c3428a8f71ba742363b"

[run_info]
docker_image = "python:3.11"
volumes_to_mount = [ "{proj_path}:/app", "{host}/.m2:/.m2", "{host}/.cache/pip:/.pip_cache", "{host}/.cache/go-build:/.go_cache", "{proj_path}/_HOME_/go:/go",]
docker_wrap = "sudo docker run --rm -w /app -e PATH=_HOME_/.local/bin:$PATH -e PYTHONUSERBASE=_HOME_/.local/ -v {proj_path}:/app -v {host}/.m2:/.m2 -v {host}/.cache/pip:/.pip_cache -v {host}/.cache/go-build:/.go_cache -v {proj_path}/_HOME_/go:/go {img} sh -c '{cmd}'"
env = [ "PATH=_HOME_/.local/bin:$PATH", "PYTHONUSERBASE=_HOME_/.local/",]
prebuild_command = "pip install -r requirements.txt && (pip install git+https://github.com/Klema17/mutpy.git && pip install coverage pytest pytest_cov covdefaults Cython mock ddt pytest_mock testfixtures)"
test_run_command = "coverage run --include=langfun/core/llms/deepseek.py -m pytest -q --junit-xml=test_output.xml langfun/core/llms/deepseek_test.py && coverage xml -o coverage.xml --fail-under=0"
mutation_run_command = "mut.py --target langfun.core.llms.deepseek --unit-test langfun.core.llms.deepseek_test --runner pytest --report mutation_report.yaml"
mutation_run_command_fallback = "mut.py --target langfun/core/llms/deepseek.py --unit-test langfun/core/llms/deepseek_test.py --runner pytest --report mutation_report.yaml"
coverage_report_path = "coverage.xml"
coverage_report_type = "cobertura"
mutation_report_path = "mutation_report.yaml"
mutation_report_type = "mutpy"

[coverage]
coverage = 77.0
original_coverage = 100.0
mutation_kill_rate = nan
original_mutation_kill_rate = 100.0
covered_lines = [ 15, 16, 17, 18, 20, 21, 22, 25, 28, 35, 37, 43, 89, 94, 95, 98, 105, 107, 115, 116, 129, 130, 133, 142, 143, 144, 147, 154, 157, 164, 167, 169, 170, 172,]
missed_lines = [ 117, 118, 119, 123, 124, 127, 131, 138, 139, 140,]

[input_info]
test_file_path = "langfun/core/llms/deepseek_test.py"
focal_file_path = "langfun/core/llms/deepseek.py"
test_file_url = "https://github.com/google/langfun/blob/11d646e44baaea09bb792c3428a8f71ba742363b/langfun/core/llms/deepseek_test.py"
focal_file_url = "https://github.com/google/langfun/blob/11d646e44baaea09bb792c3428a8f71ba742363b/langfun/core/llms/deepseek.py"
first_commit_date = "2025-01-08"
last_commit_date = "2025-02-12"
test_file_content = "# Copyright 2023 The Langfun Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tests for OpenAI models.\"\"\"\n\nfrom typing import Any\nimport unittest\nfrom unittest import mock\n\nimport langfun.core as lf\nfrom langfun.core.llms import deepseek\nimport pyglove as pg\nimport requests\n\n\ndef mock_chat_completion_request(url: str, json: dict[str, Any], **kwargs):\n  del url, kwargs\n  messages = json['messages']\n  if len(messages) > 1:\n    system_message = f' system={messages[0][\"content\"]}'\n  else:\n    system_message = ''\n\n  if 'response_format' in json:\n    response_format = f' format={json[\"response_format\"][\"type\"]}'\n  else:\n    response_format = ''\n\n  choices = []\n  for k in range(json['n']):\n    if json.get('logprobs'):\n      logprobs = dict(\n          content=[\n              dict(\n                  token='chosen_token',\n                  logprob=0.5,\n                  top_logprobs=[\n                      dict(\n                          token=f'alternative_token_{i + 1}',\n                          logprob=0.1\n                      ) for i in range(3)\n                  ]\n              )\n          ]\n      )\n    else:\n      logprobs = None\n\n    choices.append(dict(\n        message=dict(\n            content=(\n                f'Sample {k} for message.{system_message}{response_format}'\n            )\n        ),\n        logprobs=logprobs,\n    ))\n  response = requests.Response()\n  response.status_code = 200\n  response._content = pg.to_json_str(\n      dict(\n          choices=choices,\n          usage=lf.LMSamplingUsage(\n              prompt_tokens=100,\n              completion_tokens=100,\n              total_tokens=200,\n          ),\n      )\n  ).encode()\n  return response\n\n\nclass DeepSeekTest(unittest.TestCase):\n  \"\"\"Tests for DeepSeek language model.\"\"\"\n\n  def test_dir(self):\n    self.assertIn('deepseek-chat', deepseek.DeepSeek.dir())\n\n  def test_key(self):\n    with self.assertRaisesRegex(ValueError, 'Please specify `api_key`'):\n      deepseek.DeepSeekChat()('hi')\n\n  def test_model_id(self):\n    self.assertEqual(\n        deepseek.DeepSeekChat(api_key='test_key').model_id,\n        'DeepSeek(deepseek-chat)',\n    )\n\n  def test_resource_id(self):\n    self.assertEqual(\n        deepseek.DeepSeekChat(api_key='test_key').resource_id,\n        'DeepSeek(deepseek-chat)',\n    )\n\n  def test_max_concurrency(self):\n    self.assertGreater(\n        deepseek.DeepSeekChat(api_key='test_key').max_concurrency, 0\n    )\n\n  def test_request_args(self):\n    self.assertEqual(\n        deepseek.DeepSeekChat(api_key='test_key')._request_args(\n            lf.LMSamplingOptions(\n                temperature=1.0, stop=['\\n'], n=1, random_seed=123\n            )\n        ),\n        dict(\n            model='deepseek-chat',\n            top_logprobs=None,\n            n=1,\n            temperature=1.0,\n            stop=['\\n'],\n            seed=123,\n        ),\n    )\n\n  def test_call_chat_completion(self):\n    with mock.patch('requests.Session.post') as mock_request:\n      mock_request.side_effect = mock_chat_completion_request\n      lm = deepseek.DeepSeek(model='deepseek-chat', api_key='test_key')\n      self.assertEqual(\n          lm('hello', sampling_options=lf.LMSamplingOptions(n=2)),\n          'Sample 0 for message.',\n      )\n\n  def test_call_chat_completion_with_logprobs(self):\n    with mock.patch('requests.Session.post') as mock_request:\n      mock_request.side_effect = mock_chat_completion_request\n      lm = deepseek.DeepSeek(model='deepseek-chat', api_key='test_key')\n      results = lm.sample(['hello'], logprobs=True)\n      self.assertEqual(len(results), 1)\n      expected = lf.LMSamplingResult(\n          [\n              lf.LMSample(\n                  response=lf.AIMessage(\n                      text='Sample 0 for message.',\n                      metadata={\n                          'score': 0.0,\n                          'logprobs': [(\n                              'chosen_token',\n                              0.5,\n                              [\n                                  ('alternative_token_1', 0.1),\n                                  ('alternative_token_2', 0.1),\n                                  ('alternative_token_3', 0.1),\n                              ],\n                          )],\n                          'is_cached': False,\n                          'usage': lf.LMSamplingUsage(\n                              prompt_tokens=100,\n                              completion_tokens=100,\n                              total_tokens=200,\n                              estimated_cost=4.2e-05,\n                          ),\n                      },\n                      tags=['lm-response'],\n                  ),\n                  logprobs=[(\n                      'chosen_token',\n                      0.5,\n                      [\n                          ('alternative_token_1', 0.1),\n                          ('alternative_token_2', 0.1),\n                          ('alternative_token_3', 0.1),\n                      ],\n                  )],\n              )\n          ],\n          usage=lf.LMSamplingUsage(\n              prompt_tokens=100,\n              completion_tokens=100,\n              total_tokens=200,\n              estimated_cost=4.2e-05,\n          ),\n      )\n      self.assertTrue(pg.eq(results[0], expected))\n\n  def test_sample_chat_completion(self):\n    with mock.patch('requests.Session.post') as mock_request:\n      mock_request.side_effect = mock_chat_completion_request\n      deepseek.SUPPORTED_MODELS_AND_SETTINGS['deepseek-chat'].update({\n          'cost_per_1k_input_tokens': 1.0,\n          'cost_per_1k_output_tokens': 1.0,\n      })\n      lm = deepseek.DeepSeek(api_key='test_key', model='deepseek-chat')\n      results = lm.sample(\n          ['hello', 'bye'], sampling_options=lf.LMSamplingOptions(n=3)\n      )\n\n    self.assertEqual(len(results), 2)\n    print(results[0])\n    self.assertEqual(\n        results[0],\n        lf.LMSamplingResult(\n            [\n                lf.LMSample(\n                    lf.AIMessage(\n                        'Sample 0 for message.',\n                        score=0.0,\n                        logprobs=None,\n                        is_cached=False,\n                        usage=lf.LMSamplingUsage(\n                            prompt_tokens=33,\n                            completion_tokens=33,\n                            total_tokens=66,\n                            estimated_cost=0.2 / 3,\n                        ),\n                        tags=[lf.Message.TAG_LM_RESPONSE],\n                    ),\n                    score=0.0,\n                    logprobs=None,\n                ),\n                lf.LMSample(\n                    lf.AIMessage(\n                        'Sample 1 for message.',\n                        score=0.0,\n                        logprobs=None,\n                        is_cached=False,\n                        usage=lf.LMSamplingUsage(\n                            prompt_tokens=33,\n                            completion_tokens=33,\n                            total_tokens=66,\n                            estimated_cost=0.2 / 3,\n                        ),\n                        tags=[lf.Message.TAG_LM_RESPONSE],\n                    ),\n                    score=0.0,\n                    logprobs=None,\n                ),\n                lf.LMSample(\n                    lf.AIMessage(\n                        'Sample 2 for message.',\n                        score=0.0,\n                        logprobs=None,\n                        is_cached=False,\n                        usage=lf.LMSamplingUsage(\n                            prompt_tokens=33,\n                            completion_tokens=33,\n                            total_tokens=66,\n                            estimated_cost=0.2 / 3,\n                        ),\n                        tags=[lf.Message.TAG_LM_RESPONSE],\n                    ),\n                    score=0.0,\n                    logprobs=None,\n                ),\n            ],\n            usage=lf.LMSamplingUsage(\n                prompt_tokens=100, completion_tokens=100, total_tokens=200,\n                estimated_cost=0.2,\n            ),\n        ),\n    )\n    self.assertEqual(\n        results[1],\n        lf.LMSamplingResult(\n            [\n                lf.LMSample(\n                    lf.AIMessage(\n                        'Sample 0 for message.',\n                        score=0.0,\n                        logprobs=None,\n                        is_cached=False,\n                        usage=lf.LMSamplingUsage(\n                            prompt_tokens=33,\n                            completion_tokens=33,\n                            total_tokens=66,\n                            estimated_cost=0.2 / 3,\n                        ),\n                        tags=[lf.Message.TAG_LM_RESPONSE],\n                    ),\n                    score=0.0,\n                    logprobs=None,\n                ),\n                lf.LMSample(\n                    lf.AIMessage(\n                        'Sample 1 for message.',\n                        score=0.0,\n                        logprobs=None,\n                        is_cached=False,\n                        usage=lf.LMSamplingUsage(\n                            prompt_tokens=33,\n                            completion_tokens=33,\n                            total_tokens=66,\n                            estimated_cost=0.2 / 3,\n                        ),\n                        tags=[lf.Message.TAG_LM_RESPONSE],\n                    ),\n                    score=0.0,\n                    logprobs=None,\n                ),\n                lf.LMSample(\n                    lf.AIMessage(\n                        'Sample 2 for message.',\n                        score=0.0,\n                        logprobs=None,\n                        is_cached=False,\n                        usage=lf.LMSamplingUsage(\n                            prompt_tokens=33,\n                            completion_tokens=33,\n                            total_tokens=66,\n                            estimated_cost=0.2 / 3,\n                        ),\n                        tags=[lf.Message.TAG_LM_RESPONSE],\n                    ),\n                    score=0.0,\n                    logprobs=None,\n                ),\n            ],\n            usage=lf.LMSamplingUsage(\n                prompt_tokens=100, completion_tokens=100, total_tokens=200,\n                estimated_cost=0.2,\n            ),\n        ),\n    )\n\n  def test_sample_with_contextual_options(self):\n    with mock.patch('requests.Session.post') as mock_request:\n      mock_request.side_effect = mock_chat_completion_request\n      lm = deepseek.DeepSeek(api_key='test_key', model='deepseek-chat')\n      with lf.use_settings(sampling_options=lf.LMSamplingOptions(n=2)):\n        results = lm.sample(['hello'])\n\n    self.assertEqual(len(results), 1)\n    expected = lf.LMSamplingResult(\n        samples=[\n            lf.LMSample(\n                response=lf.AIMessage(\n                    text='Sample 0 for message.',\n                    sender='AI',\n                    metadata=pg.Dict(\n                        score=0.0,\n                        logprobs=None,\n                        is_cached=False,\n                        usage=lf.LMSamplingUsage(\n                            prompt_tokens=50,\n                            completion_tokens=50,\n                            total_tokens=100,\n                            num_requests=1,\n                            estimated_cost=0.1,\n                        ),\n                    ),\n                    tags=['lm-response'],\n                ),\n                score=0.0,\n                logprobs=None,\n            ),\n            lf.LMSample(\n                response=lf.AIMessage(\n                    text='Sample 1 for message.',\n                    sender='AI',\n                    metadata=pg.Dict(\n                        score=0.0,\n                        logprobs=None,\n                        is_cached=False,\n                        usage=lf.LMSamplingUsage(\n                            prompt_tokens=50,\n                            completion_tokens=50,\n                            total_tokens=100,\n                            num_requests=1,\n                            estimated_cost=0.1,\n                        ),\n                    ),\n                    tags=['lm-response'],\n                ),\n                score=0.0,\n                logprobs=None,\n            ),\n        ],\n        usage=lf.LMSamplingUsage(\n            prompt_tokens=100,\n            completion_tokens=100,\n            total_tokens=200,\n            num_requests=1,\n            estimated_cost=0.2,\n        ),\n        is_cached=False,\n    )\n    self.assertTrue(pg.eq(results[0], expected))\n\n  def test_call_with_system_message(self):\n    with mock.patch('requests.Session.post') as mock_request:\n      mock_request.side_effect = mock_chat_completion_request\n      lm = deepseek.DeepSeek(api_key='test_key', model='deepseek-chat')\n      self.assertEqual(\n          lm(\n              lf.UserMessage(\n                  'hello',\n                  system_message='hi',\n              ),\n              sampling_options=lf.LMSamplingOptions(n=2)\n          ),\n          '''Sample 0 for message. system=[{'type': 'text', 'text': 'hi'}]''',\n      )\n\n  def test_call_with_json_schema(self):\n    with mock.patch('requests.Session.post') as mock_request:\n      mock_request.side_effect = mock_chat_completion_request\n      lm = deepseek.DeepSeek(api_key='test_key', model='deepseek-chat')\n      self.assertEqual(\n          lm(\n              lf.UserMessage(\n                  'hello',\n                  json_schema={\n                      'type': 'object',\n                      'properties': {\n                          'name': {'type': 'string'},\n                      },\n                      'required': ['name'],\n                      'title': 'Person',\n                  }\n              ),\n              sampling_options=lf.LMSamplingOptions(n=2)\n          ),\n          'Sample 0 for message. format=json_schema',\n      )\n\n    # Test bad json schema.\n    with self.assertRaisesRegex(ValueError, '`json_schema` must be a dict'):\n      lm(lf.UserMessage('hello', json_schema='foo'))\n\n    with self.assertRaisesRegex(\n        ValueError, 'The root of `json_schema` must have a `title` field'\n    ):\n      lm(lf.UserMessage('hello', json_schema={}))\n\n\nif __name__ == '__main__':\n  unittest.main()"
