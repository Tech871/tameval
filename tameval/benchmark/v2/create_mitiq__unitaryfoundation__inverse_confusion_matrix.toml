[meta]
task = "create"
scenario = "extend_test_file"

[lang_info]
lang = "Python"
python_version = "3.12"
python_cfg_file = "pyproject.toml"

[repo_info]
repository = "unitaryfoundation/mitiq"
sha = "3d4516eb01e0c82220a829bbf570ac537143b5a5"

[run_info]
docker_image = "python:3.12"
volumes_to_mount = [ "{proj_path}:/app", "{host}/.m2:/.m2", "{host}/.cache/pip:/.pip_cache", "{host}/.cache/go-build:/.go_cache", "{proj_path}/_HOME_/go:/go",]
docker_wrap = "sudo docker run --rm -w /app -e PATH=_HOME_/.local/bin:$PATH -e PYTHONUSERBASE=_HOME_/.local/ -v {proj_path}:/app -v {host}/.m2:/.m2 -v {host}/.cache/pip:/.pip_cache -v {host}/.cache/go-build:/.go_cache -v {proj_path}/_HOME_/go:/go {img} sh -c '{cmd}'"
env = [ "PATH=_HOME_/.local/bin:$PATH", "PYTHONUSERBASE=_HOME_/.local/",]
prebuild_command = "(pip install .[all,test] && pip install git+https://github.com/Klema17/mutpy.git && pip install coverage pytest pytest_cov covdefaults Cython mock ddt pytest_mock testfixtures)"
test_run_command = "coverage run --include=mitiq/rem/inverse_confusion_matrix.py -m pytest -q --junit-xml=test_output.xml mitiq/rem/tests/test_inverse_confusion_matrix.py && coverage xml -o coverage.xml --fail-under=0"
mutation_run_command = "mut.py --target mitiq.rem.inverse_confusion_matrix --unit-test mitiq.rem.tests.test_inverse_confusion_matrix --runner pytest --report mutation_report.yaml"
mutation_run_command_fallback = "mut.py --target mitiq/rem/inverse_confusion_matrix.py --unit-test mitiq/rem/tests/test_inverse_confusion_matrix.py --runner pytest --report mutation_report.yaml"
coverage_report_path = "coverage.xml"
coverage_report_type = "cobertura"
mutation_report_path = "mutation_report.yaml"
mutation_report_type = "mutpy"

[coverage]
coverage = 97.0
original_coverage = 97.0
mutation_kill_rate = 88.0
original_mutation_kill_rate = 88.0
covered_lines = [ 5, 6, 8, 9, 10, 12, 15, 32, 33, 34, 38, 42, 43, 45, 48, 61, 62, 63, 64, 65, 67, 70, 91, 92, 94, 95, 98, 115, 116, 118, 119, 120, 126, 129, 142, 143, 144, 146, 147, 149, 150, 151, 152, 158, 159, 162, 165, 180, 183, 184, 185, 191, 192, 193, 194, 197, 199,]
missed_lines = [ 181, 186,]

[input_info]
test_file_path = "mitiq/rem/tests/test_inverse_confusion_matrix.py"
focal_file_path = "mitiq/rem/inverse_confusion_matrix.py"
test_file_url = "https://github.com/unitaryfoundation/mitiq/blob/3d4516eb01e0c82220a829bbf570ac537143b5a5/mitiq/rem/tests/test_inverse_confusion_matrix.py"
focal_file_url = "https://github.com/unitaryfoundation/mitiq/blob/3d4516eb01e0c82220a829bbf570ac537143b5a5/mitiq/rem/inverse_confusion_matrix.py"
first_commit_date = "2022-11-22"
last_commit_date = "2025-06-07"
test_file_content = "# Copyright (C) Unitary Foundation\n#\n# This source code is licensed under the GPL license (v3) found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"Unit tests for inverse confusion matrix helper functions.\"\"\"\n\nfrom functools import reduce\nfrom math import isclose\n\nimport numpy as np\nimport pytest\n\nfrom mitiq import MeasurementResult\nfrom mitiq.rem.inverse_confusion_matrix import (\n    bitstrings_to_probability_vector,\n    closest_positive_distribution,\n    generate_inverse_confusion_matrix,\n    generate_tensored_inverse_confusion_matrix,\n    mitigate_measurements,\n    sample_probability_vector,\n)\n\n\ndef test_sample_probability_vector_invalid_size():\n    with pytest.raises(ValueError, match=\"power of 2\"):\n        sample_probability_vector([1 / 3, 1 / 3, 1 / 3], 3)\n\n\ndef test_sample_probability_vector_single_qubit():\n    bitstrings = sample_probability_vector(np.array([1, 0]), 10)\n    assert all(b == \"0\" for b in bitstrings)\n\n    bitstrings = sample_probability_vector(np.array([0, 1]), 10)\n    assert all(b == \"1\" for b in bitstrings)\n\n    np.random.seed(0)\n    bitstrings = sample_probability_vector(np.array([0.5, 0.5]), 1000)\n    assert sum(int(b) for b in bitstrings) == 483\n\n\ndef test_sample_probability_vector_two_qubits():\n    bitstrings = sample_probability_vector(np.array([1, 0, 0, 0]), 10)\n    assert all(b == \"00\" for b in bitstrings)\n\n    bitstrings = sample_probability_vector(np.array([0, 1, 0, 0]), 10)\n    assert all(b == \"01\" for b in bitstrings)\n\n    bitstrings = sample_probability_vector(np.array([0, 0, 1, 0]), 10)\n    assert all(b == \"10\" for b in bitstrings)\n\n    bitstrings = sample_probability_vector(np.array([0, 0, 0, 1]), 10)\n    assert all(b == \"11\" for b in bitstrings)\n\n\ndef test_bitstrings_to_probability_vector():\n    pv = bitstrings_to_probability_vector([[0]])\n    assert (pv == np.array([1, 0])).all()\n\n    pv = bitstrings_to_probability_vector([[1]])\n    assert (pv == np.array([0, 1])).all()\n\n    pv = bitstrings_to_probability_vector([[0], [1]])\n    assert (pv == np.array([0.5, 0.5])).all()\n\n    pv = bitstrings_to_probability_vector([[0, 0]])\n    assert (pv == np.array([1, 0, 0, 0])).all()\n\n    pv = bitstrings_to_probability_vector([[1, 1]])\n    assert (pv == np.array([0, 0, 0, 1])).all()\n\n\n@pytest.mark.parametrize(\"_\", range(10))\ndef test_probability_vector_roundtrip(_):\n    pv = np.random.rand(4)\n    pv /= np.sum(pv)\n    assert isclose(\n        np.linalg.norm(\n            pv\n            - bitstrings_to_probability_vector(\n                sample_probability_vector(pv, 1000)\n            )\n        ),\n        0,\n        abs_tol=0.1,\n    )\n\n\ndef test_generate_inverse_confusion_matrix():\n    num_qubits = 2\n    identity = np.identity(4)\n    assert (\n        generate_inverse_confusion_matrix(num_qubits, p0=0, p1=0) == identity\n    ).all()\n    assert (\n        generate_inverse_confusion_matrix(num_qubits, p0=1, p1=1)\n        == np.flipud(identity)\n    ).all()\n\n\n@pytest.mark.parametrize(\n    \"num_qubits, confusion_matrices, expected\",\n    [\n        (2, [np.identity(2), np.identity(2)], np.identity(4)),\n        (2, [np.identity(4)], np.identity(4)),\n        (3, [np.identity(4), np.identity(2)], np.identity(8)),\n        # all are faulty qubits, flipping to opposite value\n        (\n            2,\n            [np.flipud(np.identity(2)), np.flipud(np.identity(2))],\n            np.flipud(np.identity(4)),\n        ),\n        (\n            3,\n            [np.flipud(np.identity(4)), np.flipud(np.identity(2))],\n            np.flipud(np.identity(8)),\n        ),\n        # wrongly sized confusion matrices\n        (3, [np.identity(2), np.identity(2)], ValueError),\n        # one qubit flips values, one is perfect, one is random\n        (\n            3,\n            [np.flipud(np.identity(2)), np.identity(2), 0.5 * np.ones((2, 2))],\n            np.linalg.pinv(\n                reduce(\n                    np.kron,\n                    [\n                        np.flipud(np.identity(2)),\n                        np.identity(2),\n                        0.5 * np.ones((2, 2)),\n                    ],\n                )\n            ),\n        ),\n    ],\n)\ndef test_generate_tensored_inverse_confusion_matrix(\n    num_qubits, confusion_matrices, expected\n):\n    if expected is ValueError:\n        with pytest.raises(ValueError):\n            generate_tensored_inverse_confusion_matrix(\n                num_qubits, confusion_matrices\n            )\n    else:\n        assert np.allclose(\n            generate_tensored_inverse_confusion_matrix(\n                num_qubits, confusion_matrices\n            ),\n            expected,\n        )\n\n\ndef test_mitigate_measurements():\n    identity = np.identity(4)\n\n    measurements = MeasurementResult([[1, 0]])\n    assert mitigate_measurements(measurements, identity) == measurements\n    assert mitigate_measurements(measurements, np.flipud(identity)).result == [\n        [0, 1]\n    ]\n\n    measurements = MeasurementResult([[0, 1]])\n    assert mitigate_measurements(measurements, identity) == measurements\n    assert mitigate_measurements(measurements, np.flipud(identity)).result == [\n        [1, 0]\n    ]\n\n\ndef test_closest_positive_distribution():\n    inputs = [\n        [0.3, 0.7],  # Test optimal input\n        [-0.1, 1.1],  # Test negative elements\n        [10, 10],  # Test normalization\n        [-1, 1, -1, 1],  # Test more elements\n        [-1, 0.1, -1, 0.2],  # Non-trivial problem\n    ]\n    expected = [\n        [0.3, 0.7],\n        [0, 1],\n        [0.5, 0.5],\n        [0, 0.5, 0, 0.5],\n        [0, 0.450317, 0, 0.549683],\n    ]\n    for quasi_prob, prob in zip(inputs, expected):\n        assert np.allclose(\n            closest_positive_distribution(quasi_prob),\n            prob,\n            atol=1e-5,\n        )\n\n\n@pytest.mark.filterwarnings(\"ignore:invalid value encountered in divide\")\ndef test_closest_positive_distribution_error():\n    \"\"\"Test unfeasible problem to trigger error.\"\"\"\n    with pytest.raises(ValueError, match=\"REM failed to determine\"):\n        closest_positive_distribution([0, 0])\n"
