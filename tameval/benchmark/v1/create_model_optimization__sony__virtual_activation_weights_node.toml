[meta]
task = "create"
scenario = "add_new_test"

[lang_info]
lang = "Python"
python_cfg_file = "requirements.txt"

[repo_info]
repository = "sony/model_optimization"
sha = "0c4bc5984cea42c5cde2b05d4f3032596b054a4c"

[run_info]
docker_image = "python:3"
volumes_to_mount = [ "{proj_path}:/app", "{host}/.m2:/.m2", "{host}/.cache/pip:/.pip_cache", "{host}/.cache/go-build:/.go_cache", "{proj_path}/_HOME_/go:/go",]
docker_wrap = "sudo docker run --rm -w /app -e PATH=_HOME_/.local/bin:$PATH -e PYTHONUSERBASE=_HOME_/.local/ -v {proj_path}:/app -v {host}/.m2:/.m2 -v {host}/.cache/pip:/.pip_cache -v {host}/.cache/go-build:/.go_cache -v {proj_path}/_HOME_/go:/go {img} sh -c '{cmd}'"
env = [ "PATH=_HOME_/.local/bin:$PATH", "PYTHONUSERBASE=_HOME_/.local/",]
prebuild_command = "pip install -r requirements.txt && (pip install git+https://github.com/Klema17/mutpy.git && pip install coverage pytest pytest_cov covdefaults Cython mock ddt pytest_mock testfixtures)"
test_run_command = "coverage run --include=model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py -m pytest -q --junit-xml=test_output.xml tests_pytest/common_tests/unit_tests/core/graph/test_virtual_activation_weights_node.py && coverage xml -o coverage.xml --fail-under=0"
mutation_run_command = "mut.py --target model_compression_toolkit.core.common.graph.virtual_activation_weights_node --unit-test tests_pytest.common_tests.unit_tests.core.graph.test_virtual_activation_weights_node --runner pytest --report mutation_report.yaml"
mutation_run_command_fallback = "mut.py --target model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py --unit-test tests_pytest/common_tests/unit_tests/core/graph/test_virtual_activation_weights_node.py --runner pytest --report mutation_report.yaml"
coverage_report_path = "coverage.xml"
coverage_report_type = "cobertura"
mutation_report_path = "mutation_report.yaml"
mutation_report_type = "mutpy"

[coverage]
coverage = 76.0
original_coverage = 76.0
mutation_kill_rate = 16.0
original_mutation_kill_rate = 16.0
covered_lines = [ 14, 16, 18, 19, 22, 23, 25, 29, 34, 56, 63, 85, 92, 115, 128, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 154, 155, 158, 159, 160, 161, 166, 167, 168, 169, 171, 172, 183, 184, 186, 187, 188, 189, 190, 192, 194, 198, 202, 205, 208,]
missed_lines = [ 42, 53, 72, 74, 82, 100, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112,]

[input_info]
test_file_path = "tests_pytest/common_tests/unit_tests/core/graph/test_virtual_activation_weights_node.py"
focal_file_path = "model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py"
test_file_url = "https://github.com/sony/model_optimization/blob/0c4bc5984cea42c5cde2b05d4f3032596b054a4c/tests_pytest/common_tests/unit_tests/core/graph/test_virtual_activation_weights_node.py"
focal_file_url = "https://github.com/sony/model_optimization/blob/0c4bc5984cea42c5cde2b05d4f3032596b054a4c/model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py"
first_commit_date = "2022-08-14"
last_commit_date = "2025-03-25"
test_file_content = "# Copyright 2025 Sony Semiconductor Israel, Inc. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport numpy as np\nimport pytest\n\nfrom model_compression_toolkit.core.common.graph.virtual_activation_weights_node import VirtualActivationWeightsNode\nfrom tests_pytest._test_util.graph_builder_utils import build_node, DummyLayer, build_nbits_qc\n\n\nclass DummyLayerWKernel:\n    pass\n\n\nclass TestVirtualActivationWeightsNode:\n    # TODO tests only cover combining weights from activation and weight nodes and errors.\n    def test_activation_with_weights(self, fw_info_mock):\n        \"\"\" Tests that weights from activation and weight node are combined correctly. \"\"\"\n        # Each node has a unique weight attr and a unique positional weights. In addition, both nodes have\n        # an identical canonical attribute (but different full name), and an identical positional weight.\n        # All weights have different quantization.\n        a_node = build_node('a', layer_class=DummyLayer,\n                            final_weights={'aaweightaa': np.ones((3, 14)), 'foo': np.ones(15),\n                                           1: np.ones(15), 2: np.ones((5, 9))},\n                            qcs=[build_nbits_qc(a_nbits=5,\n                                                w_attr={'aaweightaa': (2, True), 'foo': (3, True)},\n                                                pos_attr=(4, True, [1, 2]),\n                                                convert_canonical_attr=False)])\n        w_node = build_node('w', layer_class=DummyLayerWKernel,\n                            final_weights={'wwweightww': np.ones((2, 71)), 'bar': np.ones(8),\n                                           1: np.ones(28), 3: np.ones(18)},\n                            qcs=[build_nbits_qc(a_nbits=6,\n                                                w_attr={'wwweightww': (5, True), 'bar': (6, True)},\n                                                pos_attr=(7, True, [1, 3]),\n                                                convert_canonical_attr=False)])\n\n        fw_info_mock.get_kernel_op_attributes = lambda nt: ['weight'] if nt is DummyLayerWKernel else [None]\n\n        v_node = VirtualActivationWeightsNode(a_node, w_node, fw_info_mock)\n        assert len(v_node.weights) == 8\n\n        assert len(w_node.weights) == len(a_node.weights) == 4\n        # weights from weight node are unchanged\n        for k, v in w_node.weights.items():\n            assert np.array_equal(v_node.weights.pop(k), v)\n        # unique weights from activation node are unchanged\n        assert np.array_equal(v_node.weights.pop('foo'), a_node.weights['foo'])\n        assert np.array_equal(v_node.weights.pop(2), a_node.weights[2])\n        # duplicate positional weight\n        assert np.array_equal(v_node.weights.pop(101), a_node.weights[1])\n        # duplicate weight attribute\n        [(new_attr, w)] = v_node.weights.items()\n        assert 'weight' not in new_attr\n        assert np.array_equal(w, a_node.weights['aaweightaa'])\n\n        assert len(v_node.candidates_quantization_cfg) == 1\n        v_qc = v_node.candidates_quantization_cfg[0]\n        v_attr_cfg = v_qc.weights_quantization_cfg.attributes_config_mapping\n        v_pos_cfg = v_qc.weights_quantization_cfg.pos_attributes_config_mapping\n        a_qc = a_node.candidates_quantization_cfg[0]\n        w_qc = w_node.candidates_quantization_cfg[0]\n\n        assert v_attr_cfg == {\n            'wwweightww': w_qc.weights_quantization_cfg.attributes_config_mapping['wwweightww'],\n            'bar': w_qc.weights_quantization_cfg.attributes_config_mapping['bar'],\n            'foo': a_qc.weights_quantization_cfg.attributes_config_mapping['foo'],\n            new_attr: a_qc.weights_quantization_cfg.attributes_config_mapping['aaweightaa']\n        }\n        assert v_pos_cfg == {\n            1: w_qc.weights_quantization_cfg.pos_attributes_config_mapping[1],\n            101: a_qc.weights_quantization_cfg.pos_attributes_config_mapping[1],\n            2: a_qc.weights_quantization_cfg.pos_attributes_config_mapping[2],\n            3: w_qc.weights_quantization_cfg.pos_attributes_config_mapping[3]\n        }\n\n    def test_invalid_configurable_w_node_weight(self, fw_info_mock):\n        w_node = build_node('w', layer_class=DummyLayerWKernel,\n                            canonical_weights={'kernel': np.ones(3), 'foo': np.ones(14)},\n                            qcs=[\n                                build_nbits_qc(w_attr={'kernel': (8, True), 'foo': (8, True)}),\n                                build_nbits_qc(w_attr={'kernel': (8, True), 'foo': (4, True)})\n                            ])\n        a_node = build_node('a', qcs=[build_nbits_qc()])\n\n        fw_info_mock.get_kernel_op_attributes = lambda nt: ['kernel'] if nt is DummyLayerWKernel else [None]\n\n        with pytest.raises(NotImplementedError, match='Only kernel weight can be configurable. Got configurable .*foo'):\n            VirtualActivationWeightsNode(a_node, w_node, fw_info_mock)\n\n    def test_invalid_a_node_configurable_weight(self, fw_info_mock):\n        w_node = build_node('w', layer_class=DummyLayerWKernel,\n                            canonical_weights={'kernel': np.ones(3), 'foo': np.ones(14)},\n                            qcs=[\n                                build_nbits_qc(w_attr={'kernel': (8, True), 'foo': (8, True)}),\n                                build_nbits_qc(w_attr={'kernel': (4, True), 'foo': (8, True)})\n                            ])\n        a_node = build_node('aaa', canonical_weights={'bar': np.ones(3), 'baz': np.ones(14)},\n                            qcs=[\n                                build_nbits_qc(w_attr={'bar': (8, True), 'baz': (8, True)}),\n                                build_nbits_qc(w_attr={'bar': (8, True), 'baz': (4, True)})\n                            ])\n        fw_info_mock.get_kernel_op_attributes = lambda nt: ['kernel'] if nt is DummyLayerWKernel else [None]\n\n        with pytest.raises(NotImplementedError, match='Node .*aaa with a configurable weight cannot be used as '\n                                                      'activation for VirtualActivationWeightsNode'):\n            VirtualActivationWeightsNode(a_node, w_node, fw_info_mock)\n\n    def test_invalid_a_node_kernel(self, fw_info_mock):\n        w_node = build_node('w', layer_class=DummyLayerWKernel, canonical_weights={'weight': np.ones(3)},\n                            qcs=[build_nbits_qc(w_attr={'weight': (8, True)})])\n        a_node = build_node('aaa', canonical_weights={'kernel': np.ones(3)},\n                            qcs=[build_nbits_qc(w_attr={'kernel': (8, True)})])\n        fw_info_mock.get_kernel_op_attributes = lambda nt: ['weight'] if nt is DummyLayerWKernel else ['kernel']\n\n        with pytest.raises(NotImplementedError, match='Node .*aaa with kernel cannot be used as '\n                                                      'activation for VirtualActivationWeightsNode'):\n            VirtualActivationWeightsNode(a_node, w_node, fw_info_mock)\n\n"
