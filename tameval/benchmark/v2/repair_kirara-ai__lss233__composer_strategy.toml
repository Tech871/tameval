[meta]
task = "repair"
scenario = "repair_missed_asserts"

[lang_info]
lang = "Python"
python_version = "3.13"
python_cfg_file = "pyproject.toml"

[repo_info]
repository = "lss233/kirara-ai"
sha = "8295a5deda0b289a3f70d946064b6c9a3e1b0753"

[run_info]
docker_image = "python:3.13"
volumes_to_mount = [ "{proj_path}:/app", "{host}/.m2:/.m2", "{host}/.cache/pip:/.pip_cache", "{host}/.cache/go-build:/.go_cache", "{proj_path}/_HOME_/go:/go",]
docker_wrap = "sudo docker run --rm -w /app -e PATH=_HOME_/.local/bin:$PATH -e PYTHONUSERBASE=_HOME_/.local/ -v {proj_path}:/app -v {host}/.m2:/.m2 -v {host}/.cache/pip:/.pip_cache -v {host}/.cache/go-build:/.go_cache -v {proj_path}/_HOME_/go:/go {img} sh -c '{cmd}'"
env = [ "PATH=_HOME_/.local/bin:$PATH", "PYTHONUSERBASE=_HOME_/.local/",]
prebuild_command = "(pip install .[all,test] && pip install git+https://github.com/Klema17/mutpy.git && pip install coverage pytest pytest_cov covdefaults Cython mock ddt pytest_mock testfixtures)"
test_run_command = "coverage run --include=kirara_ai/memory/composes/composer_strategy.py -m pytest -q --junit-xml=test_output.xml tests/memory/test_composer_strategy.py && coverage xml -o coverage.xml --fail-under=0"
mutation_run_command = "mut.py --target kirara_ai.memory.composes.composer_strategy --unit-test tests.memory.test_composer_strategy --runner pytest --report mutation_report.yaml"
mutation_run_command_fallback = "mut.py --target kirara_ai/memory/composes/composer_strategy.py --unit-test tests/memory/test_composer_strategy.py --runner pytest --report mutation_report.yaml"
coverage_report_path = "coverage.xml"
coverage_report_type = "cobertura"
mutation_report_path = "mutation_report.yaml"
mutation_report_type = "mutpy"

[coverage]
coverage = 96.0
original_coverage = 96.0
mutation_kill_rate = 0
original_mutation_kill_rate = nan
covered_lines = [ 0, 1, 3, 4, 5, 6, 8, 10, 13, 15, 16, 19, 22, 23, 25, 26, 30, 33, 34, 37, 40, 41, 42, 44, 45, 46, 49, 52, 53, 56, 59, 60, 61, 63, 64, 65, 67, 68, 71, 74, 75, 76, 78, 82, 85, 88, 89, 91, 92, 93, 94, 107, 113, 118, 121, 124, 125, 126, 131, 132, 134, 135, 136, 137, 138, 140, 142, 145, 148, 149, 150, 157, 158, 159, 161, 162, 163, 164, 165, 167, 170, 172, 173, 175, 178, 181, 182, 183, 188, 190, 191, 192, 193,]
missed_lines = [ 98, 100, 101, 102,]

[input_info]
test_file_path = "tests/memory/test_composer_strategy.py"
focal_file_path = "kirara_ai/memory/composes/composer_strategy.py"
test_file_url = "https://github.com/lss233/kirara-ai/blob/8295a5deda0b289a3f70d946064b6c9a3e1b0753/tests/memory/test_composer_strategy.py"
focal_file_url = "https://github.com/lss233/kirara-ai/blob/8295a5deda0b289a3f70d946064b6c9a3e1b0753/kirara_ai/memory/composes/composer_strategy.py"
first_commit_date = "2025-04-20"
last_commit_date = "2025-04-20"
test_file_content = "from unittest.mock import Mock\n\nimport pytest\n\nimport kirara_ai.llm.format.tool as tools\nfrom kirara_ai.im.message import ImageMessage, IMMessage, TextMessage\nfrom kirara_ai.im.sender import ChatSender, ChatType\nfrom kirara_ai.ioc.container import DependencyContainer\nfrom kirara_ai.llm.format.message import (LLMChatImageContent, LLMChatMessage, LLMChatTextContent, LLMToolCallContent,\n                                          LLMToolResultContent)\nfrom kirara_ai.memory.composes.composer_strategy import (IMMessageProcessor, LLMChatImageContentProcessor,\n                                                         LLMChatMessageProcessor, LLMChatTextContentProcessor,\n                                                         LLMToolCallContentProcessor, LLMToolResultContentProcessor,\n                                                         MediaMessageProcessor, ProcessorFactory, TextMessageProcessor,\n                                                         drop_think_part)\n\n\n@pytest.fixture\ndef mock_container():\n    container = Mock(spec=DependencyContainer)\n    media_manager = Mock()\n    container.resolve.return_value = media_manager\n    return container\n\n\n@pytest.fixture\ndef sample_context():\n    return {\n        \"media_ids\": [],\n        \"tool_calls\": [],\n        \"tool_results\": []\n    }\n\n\nclass TestDropThinkPart:\n    def test_drop_think_part_with_think_tag(self):\n        text = \"<think>这是思考部分</think>这是输出部分\"\n        result = drop_think_part(text)\n\n    def test_drop_think_part_without_think_tag(self):\n        text = \"这是纯文本，没有思考标签\"\n        result = drop_think_part(text)\n\nclass TestTextMessageProcessor:\n    def test_process(self, mock_container, sample_context):\n        processor = TextMessageProcessor(mock_container)\n        message = TextMessage(\"这是一条文本消息\")\n\n        result = processor.process(message, sample_context)\n\n\nclass TestMediaMessageProcessor:\n    def test_process(self, mock_container, sample_context):\n        processor = MediaMessageProcessor(mock_container)\n        message = ImageMessage(media_id=\"media1\", data=b\"test\", format=\"png\")\n\n        result = processor.process(message, sample_context)\n\n\nclass TestLLMChatTextContentProcessor:\n    def test_process_normal_text(self, mock_container, sample_context):\n        processor = LLMChatTextContentProcessor(mock_container)\n        content = LLMChatTextContent(text=\"这是普通文本\")\n\n        result = processor.process(content, sample_context)\n\n    def test_process_with_think_tag(self, mock_container, sample_context):\n        processor = LLMChatTextContentProcessor(mock_container)\n        content = LLMChatTextContent(text=\"<think>思考过程</think>这是输出\")\n\n        result = processor.process(content, sample_context)\n\n\nclass TestLLMChatImageContentProcessor:\n    def test_process(self, mock_container, sample_context):\n        media_manager = mock_container.resolve.return_value\n        media = Mock()\n        media.description = \"图片描述\"\n        media_manager.get_media.return_value = media\n\n        processor = LLMChatImageContentProcessor(mock_container)\n        content = LLMChatImageContent(media_id=\"media1\")\n\n        result = processor.process(content, sample_context)\n\n\nclass TestLLMToolCallContentProcessor:\n    def test_process(self, mock_container, sample_context):\n        processor = LLMToolCallContentProcessor(mock_container)\n        content = LLMToolCallContent(\n            id=\"call1\",\n            name=\"test_function\",\n            parameters={\"arg1\": \"value1\"}\n        )\n\n        result = processor.process(content, sample_context)\n        tool_call = sample_context[\"tool_calls\"][0]\n\n\nclass TestLLMToolResultContentProcessor:\n    def test_process(self, mock_container, sample_context):\n        processor = LLMToolResultContentProcessor(mock_container)\n        content = LLMToolResultContent(\n            id=\"result1\",\n            name=\"test_result\",\n            isError=False,\n            content=[tools.TextContent(text=\"结果文本\")]\n        )\n\n        result = processor.process(content, sample_context)\n        tool_result = sample_context[\"tool_results\"][0]\n\n\nclass TestIMMessageProcessor:\n    def test_process_with_text_message(self, mock_container, sample_context):\n        text_message = TextMessage(\"这是文本消息\")\n        im_message = IMMessage(\n            sender=ChatSender(user_id=\"user1\", chat_type=ChatType.C2C, display_name=\"用户\"),\n            message_elements=[text_message]\n        )\n\n        processor = IMMessageProcessor(mock_container)\n        result = processor.process(im_message, sample_context)\n\n\n    def test_process_with_media_message(self, mock_container, sample_context):\n        media_message = ImageMessage(media_id=\"media1\", data=b\"test\", format=\"png\")\n        im_message = IMMessage(\n            sender=ChatSender(user_id=\"user1\", chat_type=ChatType.C2C, display_name=\"用户\"),\n            message_elements=[media_message]\n        )\n\n        processor = IMMessageProcessor(mock_container)\n        result = processor.process(im_message, sample_context)\n\n\n\nclass TestLLMChatMessageProcessor:\n    def test_process_with_text_content(self, mock_container, sample_context):\n        message = LLMChatMessage(\n            role=\"user\",\n            content=[LLMChatTextContent(text=\"这是文本内容\")]\n        )\n\n        processor = LLMChatMessageProcessor(mock_container)\n        result = processor.process(message, sample_context)\n\n\n    def test_process_with_mixed_content(self, mock_container, sample_context):\n        media_manager = mock_container.resolve.return_value\n        media = Mock()\n        media.description = \"图片描述\"\n        media_manager.get_media.return_value = media\n\n        message = LLMChatMessage(\n            role=\"assistant\",\n            content=[\n                LLMChatTextContent(text=\"这是文本内容\"),\n                LLMChatImageContent(media_id=\"media1\")\n            ]\n        )\n\n        processor = LLMChatMessageProcessor(mock_container)\n        result = processor.process(message, sample_context)\n\n\n    def test_process_with_tool_content(self, mock_container, sample_context):\n        message = LLMChatMessage(\n            role=\"assistant\",\n            content=[LLMToolCallContent(\n                id=\"call1\",\n                name=\"test_function\",\n                parameters={\"arg1\": \"value1\"}\n            )]\n        )\n\n        processor = LLMChatMessageProcessor(mock_container)\n        result = processor.process(message, sample_context)\n\n\n\nclass TestProcessorFactory:\n    def test_get_processor_for_im_message(self, mock_container):\n        factory = ProcessorFactory(mock_container)\n        processor = factory.get_processor(IMMessage)\n\n\n    def test_get_processor_for_llm_chat_message(self, mock_container):\n        factory = ProcessorFactory(mock_container)\n        processor = factory.get_processor(LLMChatMessage)\n\n\n    def test_get_processor_unknown_type(self, mock_container):\n        factory = ProcessorFactory(mock_container)\n        processor = factory.get_processor(object)"
