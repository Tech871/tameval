[meta]
task = "repair"
scenario = "repair_missed_asserts"

[lang_info]
lang = "Python"
python_cfg_file = "pyproject.toml"

[repo_info]
repository = "pytorch/captum"
sha = "aff7603051094012c9cf1a739a0538c38a6986b2"

[run_info]
docker_image = "python:3"
volumes_to_mount = [ "{proj_path}:/app", "{host}/.m2:/.m2", "{host}/.cache/pip:/.pip_cache", "{host}/.cache/go-build:/.go_cache", "{proj_path}/_HOME_/go:/go",]
docker_wrap = "sudo docker run --rm -w /app -e PATH=_HOME_/.local/bin:$PATH -e PYTHONUSERBASE=_HOME_/.local/ -v {proj_path}:/app -v {host}/.m2:/.m2 -v {host}/.cache/pip:/.pip_cache -v {host}/.cache/go-build:/.go_cache -v {proj_path}/_HOME_/go:/go {img} sh -c '{cmd}'"
env = [ "PATH=_HOME_/.local/bin:$PATH", "PYTHONUSERBASE=_HOME_/.local/",]
prebuild_command = "(pip install .[all,test] && pip install git+https://github.com/Klema17/mutpy.git && pip install coverage pytest pytest_cov covdefaults Cython mock ddt pytest_mock testfixtures)"
test_run_command = "coverage run --include=captum/attr/_core/feature_permutation.py -m pytest -q --junit-xml=test_output.xml tests/attr/test_feature_permutation.py && coverage xml -o coverage.xml --fail-under=0"
mutation_run_command = "mut.py --target captum.attr._core.feature_permutation --unit-test tests.attr.test_feature_permutation --runner pytest --report mutation_report.yaml"
mutation_run_command_fallback = "mut.py --target captum/attr/_core/feature_permutation.py --unit-test tests/attr/test_feature_permutation.py --runner pytest --report mutation_report.yaml"
coverage_report_path = "coverage.xml"
coverage_report_type = "cobertura"
mutation_report_path = "mutation_report.yaml"
mutation_report_type = "mutpy"

[coverage]
coverage = 95.0
original_coverage = 95.0
mutation_kill_rate = 0
original_mutation_kill_rate = 100.0
covered_lines = [ 3, 5, 6, 7, 8, 9, 10, 13, 14, 15, 17, 18, 19, 20, 22, 27, 75, 92, 93, 98, 104, 108, 109, 283, 285, 298, 312, 314, 327, 350, 358, 361, 363, 369, 371, 381, 382, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 401, 402, 404, 405, 407, 408, 411, 412, 414,]
missed_lines = [ 284, 313, 406,]

[input_info]
test_file_path = "tests/attr/test_feature_permutation.py"
focal_file_path = "captum/attr/_core/feature_permutation.py"
test_file_url = "https://github.com/pytorch/captum/blob/aff7603051094012c9cf1a739a0538c38a6986b2/tests/attr/test_feature_permutation.py"
focal_file_url = "https://github.com/pytorch/captum/blob/aff7603051094012c9cf1a739a0538c38a6986b2/captum/attr/_core/feature_permutation.py"
first_commit_date = "2020-02-01"
last_commit_date = "2025-07-07"
test_file_content = "\n\nfrom typing import Any, Callable, List, Tuple\n\nimport torch\nfrom captum.attr._core.feature_permutation import _permute_feature, FeaturePermutation\nfrom captum.testing.helpers import BaseTest\nfrom captum.testing.helpers.basic import assertTensorAlmostEqual, set_all_random_seeds\nfrom captum.testing.helpers.basic_models import BasicModelWithSparseInputs\nfrom torch import Tensor\n\n\nclass Test(BaseTest):\n    def construct_future_forward(self, original_forward: Callable) -> Callable:\n        def future_forward(*args: Any, **kwargs: Any) -> torch.futures.Future[Tensor]:\n            fut: torch.futures.Future[Tensor] = torch.futures.Future()\n            fut.set_result(original_forward(*args, **kwargs))\n            return fut\n\n        return future_forward\n\n    def _check_features_are_permuted(\n        self, inp: Tensor, perm_inp: Tensor, mask: Tensor\n    ) -> None:\n        permuted_features = mask.expand_as(inp[0])\n        unpermuted_features = permuted_features.bitwise_not()\n\n\n    def _check_perm_fn_with_mask(self, inp: Tensor, mask: Tensor) -> None:\n        perm_inp = _permute_feature(inp, mask)\n        self._check_features_are_permuted(inp, perm_inp, mask)\n\n    def test_perm_fn_single_feature(self) -> None:\n        batch_size = 2\n        sizes_to_test: List[Tuple[int, ...]] = [(10,), (4, 5), (3, 4, 5)]\n        for inp_size in sizes_to_test:\n            inp = torch.randn((batch_size,) + inp_size)\n            flat_mask = torch.zeros_like(inp[0]).flatten().bool()\n\n            num_features = inp.numel() // batch_size\n            for i in range(num_features):\n                flat_mask[i] = 1\n                self._check_perm_fn_with_mask(inp, flat_mask.view_as(inp[0]))\n                flat_mask[i] = 0\n\n    def test_perm_fn_broadcastable_masks(self) -> None:\n        batch_size = 5\n        inp_size = (3, 20, 30)\n\n        inp = torch.randn((batch_size,) + inp_size)\n        mask_sizes: List[Tuple[int, ...]] = [\n            (1, 20, 30),\n            (3, 1, 30),\n            (3, 20, 1),\n            (1, 1, 30),\n            (1, 20, 1),\n            (1,),\n            (30,),\n            (20, 30),\n            (3, 20, 30),\n        ]\n\n        for mask_size in mask_sizes:\n            mask = torch.randint(0, 2, mask_size).bool()\n\n            self._check_perm_fn_with_mask(inp, mask)\n\n    def test_single_input(self) -> None:\n        batch_size = 2\n        input_size = (6,)\n        constant_value = 10000\n\n        def forward_func(x: Tensor) -> Tensor:\n            return x.sum(dim=-1)\n\n        feature_importance = FeaturePermutation(forward_func=forward_func)\n\n        inp = torch.randn((batch_size,) + input_size)\n\n        inp[:, 0] = constant_value\n        zeros = torch.zeros_like(inp[:, 0])\n        for enable_cross_tensor_attribution in (True, False):\n            attribs = feature_importance.attribute(\n                inp,\n                enable_cross_tensor_attribution=enable_cross_tensor_attribution,\n            )\n\n    def test_simple_input_with_min_examples(self) -> None:\n        def forward_func(x: Tensor) -> Tensor:\n            return x.sum(dim=-1)\n\n        feature_importance = FeaturePermutation(forward_func=forward_func)\n        inp = torch.tensor([[1.0, 2.0]])\n\n        feature_importance._min_examples_per_batch = 1\n        with self.assertRaises(AssertionError):\n            feature_importance.attribute(inp)\n\n    def test_simple_input_with_min_examples_in_group(self) -> None:\n        def forward_func(x: Tensor) -> Tensor:\n            return x.sum(dim=-1)\n\n        feature_importance = FeaturePermutation(forward_func=forward_func)\n        inp = torch.tensor([[1.0, 2.0]])\n\n        feature_importance._min_examples_per_batch_grouped = 1\n        with self.assertRaises(AssertionError):\n            feature_importance.attribute(inp, enable_cross_tensor_attribution=True)\n\n    def test_simple_input_custom_mask_with_min_examples_in_group(self) -> None:\n        def forward_func(x1: Tensor, x2: Tensor) -> Tensor:\n            return x1.sum(dim=-1)\n\n        feature_importance = FeaturePermutation(forward_func=forward_func)\n        inp = (\n            torch.tensor([[1.0, 2.0]]),\n            torch.tensor(([3.0, 4.0], [5.0, 6.0])),\n        )\n        mask = (\n            torch.tensor([0, 0]),\n            torch.tensor([[0, 0], [0, 0]]),\n        )\n\n        feature_importance._min_examples_per_batch_grouped = 1\n        with self.assertRaises(AssertionError):\n            feature_importance.attribute(\n                inp, feature_mask=mask, enable_cross_tensor_attribution=True\n            )\n\n    def test_single_input_with_future(\n        self,\n    ) -> None:\n        batch_size = 2\n        input_size = (6,)\n        constant_value = 10000\n\n        def forward_func(x: Tensor) -> Tensor:\n            return x.sum(dim=-1)\n\n        feature_importance = FeaturePermutation(\n            forward_func=self.construct_future_forward(forward_func),\n        )\n\n        inp = torch.randn((batch_size,) + input_size)\n\n        inp[:, 0] = constant_value\n        zeros = torch.zeros_like(inp[:, 0])\n        for enable_cross_tensor_attribution in [True, False]:\n            attribs = feature_importance.attribute_future(\n                inp,\n                enable_cross_tensor_attribution=enable_cross_tensor_attribution,\n            )\n\n            attribs = attribs.wait()\n\n\n    def test_multi_input(\n        self,\n    ) -> None:\n        batch_size = 20\n        inp1_size = (5, 2)\n        inp2_size = (5, 3)\n\n        labels: Tensor = torch.randn(batch_size)\n\n        def forward_func(*x: Tensor) -> Tensor:\n            y = torch.zeros(x[0].shape[0:2])\n            for xx in x:\n                y += xx[:, :, 0] * xx[:, :, 1]\n            y = y.sum(dim=-1)\n            return torch.mean((y - labels) ** 2)\n\n        feature_importance = FeaturePermutation(forward_func=forward_func)\n\n        inp = (\n            torch.randn((batch_size,) + inp1_size),\n            torch.randn((batch_size,) + inp2_size),\n        )\n\n        feature_mask = (\n            torch.arange(inp[0][0].numel()).view_as(inp[0][0]).unsqueeze(0),\n            torch.arange(inp[0][0].numel(), inp[0][0].numel() + inp[1][0].numel())\n            .view_as(inp[1][0])\n            .unsqueeze(0),\n        )\n\n        inp[1][:, :, 1] = 4\n        for enable_cross_tensor_attribution in (True, False):\n            attribs = feature_importance.attribute(\n                inp,\n                feature_mask=feature_mask,\n                enable_cross_tensor_attribution=enable_cross_tensor_attribution,\n            )\n\n\n\n\n\n    def test_multi_input_group_across_input_tensors(\n        self,\n    ) -> None:\n        batch_size = 20\n        inp1_size = (5, 2)\n        inp2_size = (5, 3)\n\n        labels: Tensor = torch.randn(batch_size)\n\n        def forward_func(*x: Tensor) -> Tensor:\n            y = torch.zeros(x[0].shape[0:2])\n            for xx in x:\n                y += xx[:, :, 0] * xx[:, :, 1]\n            y = y.sum(dim=-1)\n            return torch.mean((y - labels) ** 2)\n\n        feature_importance = FeaturePermutation(forward_func=forward_func)\n\n        inp = (\n            torch.randn((batch_size,) + inp1_size),\n            torch.randn((batch_size,) + inp2_size),\n        )\n        feature_mask = tuple(\n            torch.zeros_like(inp_tensor[0]).unsqueeze(0) for inp_tensor in inp\n        )\n        attribs = feature_importance.attribute(\n            inp, feature_mask=feature_mask, enable_cross_tensor_attribution=True\n        )\n\n\n\n        first_elem_first_attrib = attribs[0].flatten()[0]\n        first_elem_second_attrib = attribs[1].flatten()[0]\n\n    def test_multi_input_with_future(\n        self,\n    ) -> None:\n        batch_size = 20\n        inp1_size = (5, 2)\n        inp2_size = (5, 3)\n\n        labels: Tensor = torch.randn(batch_size)\n\n        def forward_func(*x: Tensor) -> Tensor:\n            y = torch.zeros(x[0].shape[0:2])\n            for xx in x:\n                y += xx[:, :, 0] * xx[:, :, 1]\n            y = y.sum(dim=-1)\n            return torch.mean((y - labels) ** 2)\n\n        feature_importance = FeaturePermutation(\n            forward_func=self.construct_future_forward(forward_func)\n        )\n\n        inp = (\n            torch.randn((batch_size,) + inp1_size),\n            torch.randn((batch_size,) + inp2_size),\n        )\n\n        feature_mask = (\n            torch.arange(inp[0][0].numel()).view_as(inp[0][0]).unsqueeze(0),\n            torch.arange(inp[0][0].numel(), inp[0][0].numel() + inp[1][0].numel())\n            .view_as(inp[1][0])\n            .unsqueeze(0),\n        )\n\n        inp[1][:, :, 1] = 4\n\n        for enable_cross_tensor_attribution in [True, False]:\n            attribs = feature_importance.attribute_future(\n                inp,\n                feature_mask=feature_mask,\n                enable_cross_tensor_attribution=enable_cross_tensor_attribution,\n            )\n            attribs = attribs.wait()\n\n\n\n\n\n    def test_multiple_perturbations_per_eval(\n        self,\n    ) -> None:\n        perturbations_per_eval = 4\n        batch_size = 2\n        input_size = (4,)\n\n        inp = torch.randn((batch_size,) + input_size)\n\n        def forward_func(x: Tensor) -> Tensor:\n            return 1 - x\n\n        target = 1\n\n        feature_importance = FeaturePermutation(forward_func=forward_func)\n\n        attribs = feature_importance.attribute(\n            inp, perturbations_per_eval=perturbations_per_eval, target=target\n        )\n\n\n        for i in range(inp.size(1)):\n            if i == target:\n                continue\n\n        y = forward_func(inp)\n        actual_diff = torch.stack([(y[0] - y[1])[target], (y[1] - y[0])[target]])\n\n    def test_multiple_perturbations_per_eval_with_futures(\n        self,\n    ) -> None:\n        perturbations_per_eval = 4\n        batch_size = 2\n        input_size = (4,)\n\n        inp = torch.randn((batch_size,) + input_size)\n\n        def forward_func(x: Tensor) -> Tensor:\n            return 1 - x\n\n        target = 1\n\n        feature_importance = FeaturePermutation(\n            forward_func=self.construct_future_forward(forward_func)\n        )\n\n        for enable_cross_tensor_attribution in [True, False]:\n            attribs = feature_importance.attribute_future(\n                inp,\n                perturbations_per_eval=perturbations_per_eval,\n                target=target,\n                enable_cross_tensor_attribution=enable_cross_tensor_attribution,\n            )\n            attribs = attribs.wait()\n\n\n            for i in range(inp.size(1)):\n                if i == target:\n                    continue\n\n            y = forward_func(inp)\n            actual_diff = torch.stack([(y[0] - y[1])[target], (y[1] - y[0])[target]])\n\n    def test_broadcastable_masks(\n        self,\n    ) -> None:\n        def forward_func(x: Tensor) -> Tensor:\n            return x.view(x.shape[0], -1).sum(dim=-1)\n\n        batch_size = 2\n        inp = torch.randn((batch_size,) + (3, 4, 4))\n\n        feature_importance = FeaturePermutation(forward_func=forward_func)\n\n        masks = [\n            torch.tensor([0]),\n            torch.tensor([[0, 1, 2, 3]]),\n            torch.tensor([[[0, 1, 2, 3], [3, 3, 4, 5], [6, 6, 4, 6], [7, 8, 9, 10]]]),\n        ]\n        for enable_cross_tensor_attribution in (True, False):\n            for mask in masks:\n\n                attribs = feature_importance.attribute(\n                    inp,\n                    feature_mask=mask,\n                    enable_cross_tensor_attribution=enable_cross_tensor_attribution,\n                )\n\n                fm = mask.expand_as(inp[0])\n\n                features = set(mask.flatten())\n                for feature in features:\n                    m = (fm == feature).bool()\n                    attribs_for_feature = attribs[:, m]\n\n    def test_broadcastable_masks_with_future(\n        self,\n    ) -> None:\n        def forward_func(x: Tensor) -> Tensor:\n            return x.view(x.shape[0], -1).sum(dim=-1)\n\n        batch_size = 2\n        inp = torch.randn((batch_size,) + (3, 4, 4))\n\n        feature_importance = FeaturePermutation(\n            forward_func=self.construct_future_forward(forward_func)\n        )\n\n        masks = [\n            torch.tensor([0]),\n            torch.tensor([[0, 1, 2, 3]]),\n            torch.tensor([[[0, 1, 2, 3], [3, 3, 4, 5], [6, 6, 4, 6], [7, 8, 9, 10]]]),\n        ]\n\n        for enable_cross_tensor_attribution in [True, False]:\n            results = []\n            for mask in masks:\n                attribs_future = feature_importance.attribute_future(\n                    inp,\n                    feature_mask=mask,\n                    enable_cross_tensor_attribution=enable_cross_tensor_attribution,\n                )\n                results.append(attribs_future)\n\n            for idx in range(len(results)):\n                attribs = results[idx].wait()\n\n                fm = masks[idx].expand_as(inp[0])\n\n                features = set(masks[idx].flatten())\n                for feature in features:\n                    m = (fm == feature).bool()\n                    attribs_for_feature = attribs[:, m]\n\n    def test_empty_sparse_features(self) -> None:\n        model = BasicModelWithSparseInputs()\n        inp1 = torch.tensor([[1.0, -2.0, 3.0], [2.0, -1.0, 3.0]])\n        inp2 = torch.tensor([])\n        feature_importance = FeaturePermutation(model)\n        for enable_cross_tensor_attribution in (True, False):\n            attr1, attr2 = feature_importance.attribute(\n                (inp1, inp2),\n                enable_cross_tensor_attribution=enable_cross_tensor_attribution,\n            )\n\n    def test_sparse_features(self) -> None:\n        model = BasicModelWithSparseInputs()\n        inp1 = torch.tensor([[1.0, -2.0, 3.0], [2.0, -1.0, 3.0]])\n        inp2 = torch.tensor([1, 7, 2, 4, 5, 3, 6])\n\n        feature_importance = FeaturePermutation(model)\n\n        for enable_cross_tensor_attribution in [True, False]:\n            set_all_random_seeds(1234)\n            total_attr1, total_attr2 = feature_importance.attribute(\n                (inp1, inp2),\n                enable_cross_tensor_attribution=enable_cross_tensor_attribution,\n            )\n            for _ in range(50):\n                attr1, attr2 = feature_importance.attribute(\n                    (inp1, inp2),\n                    enable_cross_tensor_attribution=enable_cross_tensor_attribution,\n                )\n                total_attr1 += attr1\n                total_attr2 += attr2\n            total_attr1 /= 50\n            total_attr2 /= 50"
