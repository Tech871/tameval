# TAM-Eval

## ğŸ” Overview

TAM-Eval is an evaluation framework and benchmark for automated unit test maintenance.

This repository is associated with our ASE 2025 submission. 

## ğŸ”¥ Quick start

0. You should have [Docker](https://www.docker.com/) installed

1. Create new python env

```
python3.12 -m venv tenv
source tenv/bin/activate
```

2. Clone repo and install dependencies

```
git clone https://github.com/Tech871/tameval.git
cd tameval
pip install -r requirements.txt
```

and create `tameval/.env` with LLM_API_KEY (like in `tameval/.env_sample`)

3. An example script that reproduces the experiments can be used to run evaluation:

```
cd tameval
sh run_model_evals.sh
```

## Repo structure

```
tameval/
    â”œâ”€â”€ benchmark
    â”‚   â””â”€â”€ v1 # benchmark instances
    â”œâ”€â”€ run.py # main entrypoint
    â”œâ”€â”€ run_model_evals.sh # example run
    â”œâ”€â”€ setup/ # preparing stage scripts
    â”œâ”€â”€ generate/ # generation stage scripts
    â”œâ”€â”€ teval/ # evaluation stage scripts
    â”œâ”€â”€ download/ # dir for storing benchmark repos
    â”œâ”€â”€ outputs/  # dir for storing LLMs outputs
    â””â”€â”€ results/  # dir for storing evaluation results
```