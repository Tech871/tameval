[meta]
task = "create"
scenario = "add_new_test"

[lang_info]
lang = "Python"
python_version = "3.13"
python_cfg_file = "pyproject.toml"

[repo_info]
repository = "Franblueee/torchmil"
sha = "733d8fafb5c020313cca7fe3e5c8ce9c28d13a09"

[run_info]
docker_image = "python:3.13"
volumes_to_mount = [ "{proj_path}:/app", "{host}/.m2:/.m2", "{host}/.cache/pip:/.pip_cache", "{host}/.cache/go-build:/.go_cache", "{proj_path}/_HOME_/go:/go",]
docker_wrap = "sudo docker run --rm -w /app -e PATH=_HOME_/.local/bin:$PATH -e PYTHONUSERBASE=_HOME_/.local/ -v {proj_path}:/app -v {host}/.m2:/.m2 -v {host}/.cache/pip:/.pip_cache -v {host}/.cache/go-build:/.go_cache -v {proj_path}/_HOME_/go:/go {img} sh -c '{cmd}'"
env = [ "PATH=_HOME_/.local/bin:$PATH", "PYTHONUSERBASE=_HOME_/.local/",]
prebuild_command = "(pip install .[all,test] && pip install git+https://github.com/Klema17/mutpy.git && pip install coverage pytest pytest_cov covdefaults Cython mock ddt pytest_mock testfixtures)"
test_run_command = "coverage run --include=torchmil/nn/attention/attention_pool.py -m pytest -q --junit-xml=test_output.xml tests/nn/attention/test_attention_pool.py && coverage xml -o coverage.xml --fail-under=0"
mutation_run_command = "mut.py --target torchmil.nn.attention.attention_pool --unit-test tests.nn.attention.test_attention_pool --runner pytest --report mutation_report.yaml"
mutation_run_command_fallback = "mut.py --target torchmil/nn/attention/attention_pool.py --unit-test tests/nn/attention/test_attention_pool.py --runner pytest --report mutation_report.yaml"
coverage_report_path = "coverage.xml"
coverage_report_type = "cobertura"
mutation_report_path = "mutation_report.yaml"
mutation_report_type = "mutpy"

[coverage]
coverage = 95.0
original_coverage = 100.0
mutation_kill_rate = 100.0
original_mutation_kill_rate = 100.0
covered_lines = [ 0, 1, 3, 6, 31, 46, 47, 48, 49, 50, 52, 53, 55, 56, 57, 59, 60, 61, 62, 63, 64, 70, 86, 87, 89, 90, 91, 93, 94, 96, 97, 98, 99, 101, 103, 104, 106, 109,]
missed_lines = [ 66, 107,]

[input_info]
test_file_path = "tests/nn/attention/test_attention_pool.py"
focal_file_path = "torchmil/nn/attention/attention_pool.py"
test_file_url = "https://github.com/Franblueee/torchmil/blob/733d8fafb5c020313cca7fe3e5c8ce9c28d13a09/tests/nn/attention/test_attention_pool.py"
focal_file_url = "https://github.com/Franblueee/torchmil/blob/733d8fafb5c020313cca7fe3e5c8ce9c28d13a09/torchmil/nn/attention/attention_pool.py"
first_commit_date = "2025-02-25"
last_commit_date = "2025-06-09"
test_file_content = "import pytest\nimport torch\n\nfrom torchmil.nn.attention.attention_pool import AttentionPool\n\nclass TestAttentionPool:\n    @pytest.mark.parametrize(\n        \"in_dim, att_dim, act, gated\",\n        [(10, 128, \"tanh\", False), (15, 64, \"relu\", True), (8, 256, \"gelu\", False)],\n    )\n    def test_forward_pass(self, in_dim, att_dim, act, gated):\n        sample_input = torch.randn(2, 5, in_dim)  # batch_size, bag_size, in_dim\n        pool = AttentionPool(in_dim, att_dim, act, gated)\n        output = pool(sample_input)\n        assert output.shape == (2, in_dim)\n\n    def test_large_input_values(self):\n        in_dim = 10\n        large_input = torch.randn(2, 5, in_dim) * 1000  # Create very large input values\n        pool = AttentionPool(in_dim)\n        output = pool(large_input)\n        assert output.shape == (2, in_dim)\n"
