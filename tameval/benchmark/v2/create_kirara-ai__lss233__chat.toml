[meta]
task = "create"
scenario = "add_new_test"

[lang_info]
lang = "Python"
python_version = "3.13"
python_cfg_file = "pyproject.toml"

[repo_info]
repository = "lss233/kirara-ai"
sha = "8295a5deda0b289a3f70d946064b6c9a3e1b0753"

[run_info]
docker_image = "python:3.13"
volumes_to_mount = [ "{proj_path}:/app", "{host}/.m2:/.m2", "{host}/.cache/pip:/.pip_cache", "{host}/.cache/go-build:/.go_cache", "{proj_path}/_HOME_/go:/go",]
docker_wrap = "sudo docker run --rm -w /app -e PATH=_HOME_/.local/bin:$PATH -e PYTHONUSERBASE=_HOME_/.local/ -v {proj_path}:/app -v {host}/.m2:/.m2 -v {host}/.cache/pip:/.pip_cache -v {host}/.cache/go-build:/.go_cache -v {proj_path}/_HOME_/go:/go {img} sh -c '{cmd}'"
env = [ "PATH=_HOME_/.local/bin:$PATH", "PYTHONUSERBASE=_HOME_/.local/",]
prebuild_command = "(pip install .[all,test] && pip install git+https://github.com/Klema17/mutpy.git && pip install coverage pytest pytest_cov covdefaults Cython mock ddt pytest_mock testfixtures)"
test_run_command = "coverage run --include=kirara_ai/workflow/implementations/blocks/llm/chat.py -m pytest -q --junit-xml=test_output.xml tests/system_blocks/llm/test_chat.py && coverage xml -o coverage.xml --fail-under=0"
mutation_run_command = "mut.py --target kirara_ai.workflow.implementations.blocks.llm.chat --unit-test tests.system_blocks.llm.test_chat --runner pytest --report mutation_report.yaml"
mutation_run_command_fallback = "mut.py --target kirara_ai/workflow/implementations/blocks/llm/chat.py --unit-test tests/system_blocks/llm/test_chat.py --runner pytest --report mutation_report.yaml"
coverage_report_path = "coverage.xml"
coverage_report_type = "cobertura"
mutation_report_path = "mutation_report.yaml"
mutation_report_type = "mutpy"

[coverage]
coverage = 48.0
original_coverage = 69.0
mutation_kill_rate = 53.0
original_mutation_kill_rate = 100.0
covered_lines = [ 0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20, 25, 26, 27, 37, 42, 44, 80, 128, 129, 130, 133, 134, 136, 149, 171, 172, 173, 174, 175, 177, 193, 197, 198, 202, 207, 209, 222, 223, 224, 226, 227, 231, 234, 235, 236, 240, 241, 242, 244, 246, 248, 249, 252, 255, 257, 258, 259, 274, 276,]
missed_lines = [ 21, 22, 53, 54, 55, 58, 61, 62, 64, 65, 67, 68, 71, 72, 74, 76, 78, 88, 91, 98, 99, 101, 102, 103, 106, 108, 110, 113, 114, 116, 121, 122, 124, 125, 146, 147, 150, 151, 152, 153, 154, 155, 157, 161, 163, 164, 165, 167, 168, 178, 180, 181, 183, 184, 185, 186, 187, 188, 190, 228, 237, 253, 260, 261, 262, 263, 264, 265, 267, 270, 272, 278,]

[input_info]
test_file_path = "tests/system_blocks/llm/test_chat.py"
focal_file_path = "kirara_ai/workflow/implementations/blocks/llm/chat.py"
test_file_url = "https://github.com/lss233/kirara-ai/blob/8295a5deda0b289a3f70d946064b6c9a3e1b0753/tests/system_blocks/llm/test_chat.py"
focal_file_url = "https://github.com/lss233/kirara-ai/blob/8295a5deda0b289a3f70d946064b6c9a3e1b0753/kirara_ai/workflow/implementations/blocks/llm/chat.py"
first_commit_date = "2025-02-23"
last_commit_date = "2025-04-26"
test_file_content = "import asyncio\nimport threading\nfrom unittest.mock import MagicMock, patch\n\nimport pytest\n\nfrom kirara_ai.im.message import IMMessage, TextMessage\nfrom kirara_ai.im.sender import ChatSender\nfrom kirara_ai.ioc.container import DependencyContainer\nfrom kirara_ai.llm.format.message import LLMChatMessage, LLMChatTextContent, LLMToolResultContent\nfrom kirara_ai.llm.format.response import LLMChatResponse, Message, Usage\nfrom kirara_ai.llm.format.tool import CallableWrapper, Function, TextContent, Tool, ToolCall, ToolInputSchema\nfrom kirara_ai.llm.llm_manager import LLMManager\nfrom kirara_ai.workflow.core.execution.executor import WorkflowExecutor\nfrom kirara_ai.workflow.implementations.blocks.llm.chat import (ChatCompletion, ChatCompletionWithTools,\n                                                                ChatMessageConstructor, ChatResponseConverter)\n\ndef get_tools() -> list[Tool]:\n    async def mock_tool_invoke(tool_call: ToolCall) -> LLMToolResultContent:\n        return LLMToolResultContent(\n            id=tool_call.id,\n            name=tool_call.function.name,\n            content=[TextContent(text=\"晴天，温度25°C\")]\n        )\n\n    return [\n        Tool(\n            type=\"function\",\n            name=\"get_weather\",\n            description=\"Get the current weather in a given location\",\n            parameters=ToolInputSchema(\n                type=\"object\",\n                properties = {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n                    }\n                },\n                required=[\"location\"],\n            ),\n            invokeFunc=CallableWrapper(mock_tool_invoke)\n        )\n    ]\n\ndef get_llm_tool_calls() -> list[ToolCall]:\n    return [\n        ToolCall(\n            id = \"call_e33147bcb72525ed\",\n            function = Function(\n                name=\"get_weather\",\n                arguments={\"location\": \"San Francisco, CA\"}\n            )\n        )\n    ]\n\n# 创建模拟的 LLM 类\nclass MockLLM:\n    def chat(self, request):\n        return LLMChatResponse(\n            message=Message(\n                role=\"assistant\",\n                content=[LLMChatTextContent(text=\"这是 AI 的回复\")]\n            ),\n            model=\"gpt-3.5-turbo\",\n            usage=Usage(\n                prompt_tokens=10,\n                completion_tokens=20,\n                total_tokens=30\n            )\n        )\n\nclass MockLLMWithToolCalls:\n    def __init__(self, with_tool_calls=True):\n        self.with_tool_calls = with_tool_calls\n        self.call_count = 0\n\n    def chat(self, request):\n        self.call_count += 1\n\n        # 第一次调用返回工具调用\n        if self.with_tool_calls and self.call_count == 1:\n            return LLMChatResponse(\n                message=Message(\n                    role=\"assistant\",\n                    content=[LLMChatTextContent(text=\"我需要查询天气\")],\n                    tool_calls=get_llm_tool_calls()\n                ),\n                model=\"gpt-3.5-turbo\",\n                usage=Usage(\n                    prompt_tokens=10,\n                    completion_tokens=20, \n                    total_tokens=30\n                )\n            )\n        # 后续调用返回最终回复\n        else:\n            return LLMChatResponse(\n                message=Message(\n                    role=\"assistant\",\n                    content=[LLMChatTextContent(text=\"旧金山今天是晴天，温度25°C\")]\n                ),\n                model=\"gpt-3.5-turbo\",\n                usage=Usage(\n                    prompt_tokens=10,\n                    completion_tokens=20, \n                    total_tokens=30\n                )\n            )\n\n# 创建模拟的 LLMManager 类\nclass MockLLMManager(LLMManager):\n    def __init__(self):\n        self.mock_llm = MockLLM()\n\n    def get_llm_id_by_ability(self, ability):\n        return \"gpt-3.5-turbo\"\n\n    def get_llm(self, model_id):\n        return self.mock_llm\n\nclass MockLLMManagerWithToolCalls(LLMManager):\n    def __init__(self, with_tool_calls=True):\n        self.mock_llm = MockLLMWithToolCalls(with_tool_calls)\n\n    def get_llm_id_by_ability(self, ability):\n        return \"gpt-3.5-turbo\"\n\n    def get_llm(self, model_id):\n        return self.mock_llm\n\n@pytest.fixture\ndef container():\n    \"\"\"创建一个带有模拟 LLM 提供者的容器\"\"\"\n    container = DependencyContainer()\n\n    # 模拟 LLMManager\n    mock_llm_manager = MockLLMManager()\n\n    # 模拟 LLM\n    # mock_llm = MockLLM()\n\n    # 模拟响应\n    mock_response = LLMChatResponse(\n        message=Message(\n            role=\"assistant\",\n            content=[LLMChatTextContent(text=\"这是 AI 的回复\")]\n        ),\n        model=\"gpt-3.5-turbo\",\n        usage=Usage(\n            prompt_tokens=10,\n            completion_tokens=20,\n            total_tokens=30\n        )\n    )\n    # mock_llm.chat.return_value = mock_response\n\n    # 模拟 WorkflowExecutor\n    mock_executor = MagicMock(spec=WorkflowExecutor)\n\n    # 创建一个在新线程中运行的事件循环\n    def start_background_loop(loop):\n        asyncio.set_event_loop(loop)\n        loop.run_forever()\n\n    # 创建新的事件循环\n    new_loop = asyncio.new_event_loop()\n\n    # 在新线程中启动事件循环\n    t = threading.Thread(target=start_background_loop, args=(new_loop,), daemon=True)\n    t.start()\n\n    # 注册到容器\n    container.register(LLMManager, mock_llm_manager)\n    container.register(WorkflowExecutor, mock_executor)\n    container.register(asyncio.AbstractEventLoop, new_loop)\n\n    return container\n\ndef test_chat_completion_with_tools_no_tool_calls(container):\n    \"\"\"测试工具调用块 - 无工具调用情况\"\"\"\n\n    # 注册到容器 - 使用不会进行工具调用的模拟\n    container.register(LLMManager, MockLLMManagerWithToolCalls(with_tool_calls=False))\n\n    # 创建消息列表\n    messages = [\n        LLMChatMessage(role=\"system\", content=[LLMChatTextContent(text=\"你是一个助手\")]),\n        LLMChatMessage(role=\"user\", content=[LLMChatTextContent(text=\"你好，AI！\")])\n    ]\n\n    # 创建工具列表\n    tools = get_tools()\n\n    # 创建块\n    block = ChatCompletionWithTools(model_name=\"gpt-3.5-turbo\", max_iterations=3)\n    block.container = container\n\n    # 执行块\n    result = block.execute(msg=messages, tools=tools)\n\n    # 验证结果 - 直接返回响应，没有工具调用\n    assert \"resp\" in result\n    assert \"iteration_msgs\" in result\n    assert isinstance(result[\"resp\"], LLMChatResponse)\n    assert isinstance(result[\"iteration_msgs\"], list)\n    assert len(result[\"iteration_msgs\"]) == 0  # 无消息，因为没有工具调用\n"
