[meta]
task = "update"
scenario = "update_test"

[lang_info]
lang = "Go"
go_version = "1.22"

[repo_info]
repository = "secretflow/kuscia"
sha = "5386c4265acf18857c87aa008d13bb6fda2d828e"

[run_info]
docker_image = "golang:1.23.0"
volumes_to_mount = [ "{proj_path}:/app", "{host}/.m2:/.m2", "{host}/.cache/pip:/.pip_cache", "{host}/.cache/go-build:/.go_cache", "{proj_path}/_HOME_/go:/go",]
docker_wrap = "sudo docker run --rm -w /app -v {proj_path}:/app -v {host}/.m2:/.m2 -v {host}/.cache/pip:/.pip_cache -v {host}/.cache/go-build:/.go_cache -v {proj_path}/_HOME_/go:/go {img} sh -c '{cmd}'"
env = []
prebuild_command = "go mod download && go install github.com/zimmski/go-mutesting/cmd/go-mutesting@latest && go install github.com/jstemmer/go-junit-report@latest"
test_run_command = "go test -v -failfast -coverprofile=coverage.out pkg/scheduler/kusciascheduling/kusciascheduling.go pkg/scheduler/kusciascheduling/kusciascheduling_test.go 2>&1 && go tool cover -func=coverage.out"
mutation_run_command = "timeout {timeout} go-mutesting pkg/scheduler/kusciascheduling/kusciascheduling.go"
coverage_report_path = "coverage.out"
coverage_report_type = "go_cover"
mutation_report_path = "mutation_report.txt"
mutation_report_type = "go-mutesting"

[coverage]
coverage = 0
original_coverage = 55.0
mutation_kill_rate = nan
original_mutation_kill_rate = nan
covered_lines = []
missed_lines = []

[input_info]
test_file_path = "pkg/scheduler/kusciascheduling/kusciascheduling_test.go"
focal_file_path = "pkg/scheduler/kusciascheduling/kusciascheduling.go"
test_file_url = "https://github.com/secretflow/kuscia/blob/5386c4265acf18857c87aa008d13bb6fda2d828e/pkg/scheduler/kusciascheduling/kusciascheduling_test.go"
focal_file_url = "https://github.com/secretflow/kuscia/blob/5386c4265acf18857c87aa008d13bb6fda2d828e/pkg/scheduler/kusciascheduling/kusciascheduling.go"
first_commit_date = "2023-06-14"
last_commit_date = "2025-02-19"
test_file_content = "/*\nCopyright 2020 The Kubernetes Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage kusciascheduling\n\nimport (\n\t\"context\"\n\t\"testing\"\n\t\"time\"\n\n\tcorev1 \"k8s.io/api/core/v1\"\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\t\"k8s.io/client-go/informers\"\n\tclientsetfake \"k8s.io/client-go/kubernetes/fake\"\n\t\"k8s.io/client-go/tools/events\"\n\t\"k8s.io/kubernetes/pkg/scheduler/framework\"\n\t\"k8s.io/kubernetes/pkg/scheduler/framework/plugins/defaultbinder\"\n\t\"k8s.io/kubernetes/pkg/scheduler/framework/plugins/queuesort\"\n\tframeworkruntime \"k8s.io/kubernetes/pkg/scheduler/framework/runtime\"\n\tst \"k8s.io/kubernetes/pkg/scheduler/testing\"\n\n\tkusciaapisv1alpha1 \"github.com/secretflow/kuscia/pkg/crd/apis/kuscia/v1alpha1\"\n\tkusciaclientsetfake \"github.com/secretflow/kuscia/pkg/crd/clientset/versioned/fake\"\n\tkusciainformers \"github.com/secretflow/kuscia/pkg/crd/informers/externalversions\"\n\t\"github.com/secretflow/kuscia/pkg/scheduler/kusciascheduling/core\"\n\t\"github.com/secretflow/kuscia/test/util\"\n)\n\nfunc TestParseArgs(t *testing.T) {\n\ttests := []struct {\n\t\tname        string\n\t\tkConfig     runtime.Object\n\t\texpectedErr bool\n\t}{\n\t\t{\n\t\t\tname:        \"obj is empty\",\n\t\t\tkConfig:     nil,\n\t\t\texpectedErr: false,\n\t\t},\n\t\t{\n\t\t\tname:        \"obj type is invalid\",\n\t\t\tkConfig:     &corev1.Pod{},\n\t\t\texpectedErr: true,\n\t\t},\n\t\t{\n\t\t\tname: \"obj content type is invalid\",\n\t\t\tkConfig: &runtime.Unknown{\n\t\t\t\tTypeMeta:        runtime.TypeMeta{},\n\t\t\t\tRaw:             []byte(`{\"name\": \"KusciaScheduling\", \"args\": { \"resourceReservedSeconds\": 10}}`),\n\t\t\t\tContentEncoding: \"\",\n\t\t\t\tContentType:     \"\",\n\t\t\t},\n\t\t\texpectedErr: true,\n\t\t},\n\t\t{\n\t\t\tname: \"obj body is invalid\",\n\t\t\tkConfig: &runtime.Unknown{\n\t\t\t\tTypeMeta:        runtime.TypeMeta{},\n\t\t\t\tRaw:             []byte(`\"name\": \"KusciaScheduling\"`),\n\t\t\t\tContentEncoding: \"\",\n\t\t\t\tContentType:     \"application/json\",\n\t\t\t},\n\t\t\texpectedErr: true,\n\t\t},\n\t\t{\n\t\t\tname: \"obj is valid\",\n\t\t\tkConfig: &runtime.Unknown{\n\t\t\t\tTypeMeta:        runtime.TypeMeta{},\n\t\t\t\tRaw:             []byte(`{\"name\": \"KusciaScheduling\", \"args\": { \"resourceReservedSeconds\": 10}}`),\n\t\t\t\tContentEncoding: \"\",\n\t\t\t\tContentType:     \"application/json\",\n\t\t\t},\n\t\t\texpectedErr: false,\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\t_, err := parseArgs(tt.kConfig)\n\t\t\tif err != nil != tt.expectedErr {\n\t\t\t\tt.Errorf(\"expected %v, got %v\", tt.expectedErr, err != nil)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestEventsToRegister(t *testing.T) {\n\ttests := []struct {\n\t\tname     string\n\t\texpected bool\n\t}{\n\t\t{\n\t\t\tname:     \"result is not empty\",\n\t\t\texpected: true,\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tcs := &KusciaScheduling{}\n\t\t\tif got := cs.EventsToRegister(); len(got) > 0 != tt.expected {\n\t\t\t\tt.Errorf(\"expected %v, got %v\", tt.expected, got)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestName(t *testing.T) {\n\ttests := []struct {\n\t\tname     string\n\t\texpected bool\n\t}{\n\t\t{\n\t\t\tname:     \"result is not empty\",\n\t\t\texpected: true,\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tcs := &KusciaScheduling{}\n\t\t\tif got := cs.Name(); len(got) > 0 != tt.expected {\n\t\t\t\tt.Errorf(\"expected %v, got %v\", tt.expected, got)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestPermit(t *testing.T) {\n\tctx := context.Background()\n\tcs := kusciaclientsetfake.NewSimpleClientset()\n\ttrInformerFactory := kusciainformers.NewSharedInformerFactory(cs, 0)\n\ttrInformer := trInformerFactory.Kuscia().V1alpha1().TaskResources()\n\ttrInformerFactory.Start(ctx.Done())\n\n\ttr1 := util.MakeTaskResource(\"ns1\", \"tr2\", 2, nil)\n\ttr1.UID = \"222\"\n\ttr2 := util.MakeTaskResource(\"ns1\", \"tr2\", 2, nil)\n\ttr2.UID = \"222\"\n\ttrInformer.Informer().GetStore().Add(tr1)\n\ttrInformer.Informer().GetStore().Add(tr2)\n\n\tfakeClient := clientsetfake.NewSimpleClientset()\n\tinformerFactory := informers.NewSharedInformerFactory(fakeClient, 0)\n\tpodInformer := informerFactory.Core().V1().Pods()\n\tnsInformer := informerFactory.Core().V1().Namespaces()\n\tinformerFactory.Start(ctx.Done())\n\n\texistingPods, allNodes := util.MakeNodesAndPods(map[string]string{kusciaapisv1alpha1.TaskResourceUID: \"222\"},\n\t\tmap[string]string{kusciaapisv1alpha1.TaskResourceKey: \"tr2\"}, 60, 30)\n\tsnapshot := util.NewFakeSharedLister(existingPods, allNodes)\n\t// Compose a framework handle.\n\tregisteredPlugins := []st.RegisterPluginFunc{\n\t\tst.RegisterQueueSortPlugin(queuesort.Name, queuesort.New),\n\t\tst.RegisterBindPlugin(defaultbinder.Name, defaultbinder.New),\n\t}\n\tf, err := st.NewFramework(registeredPlugins, \"\", ctx.Done(),\n\t\tframeworkruntime.WithClientSet(fakeClient),\n\t\tframeworkruntime.WithEventRecorder(&events.FakeRecorder{}),\n\t\tframeworkruntime.WithInformerFactory(informerFactory),\n\t\tframeworkruntime.WithSnapshotSharedLister(snapshot),\n\t)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttests := []struct {\n\t\tname     string\n\t\tpods     []*corev1.Pod\n\t\texpected framework.Code\n\t}{\n\t\t{\n\t\t\tname: \"pods do not belong to any taskResource\",\n\t\t\tpods: []*corev1.Pod{\n\t\t\t\tst.MakePod().Name(\"pod1\").UID(\"pod1\").Obj()},\n\t\t\texpected: framework.Success,\n\t\t},\n\t\t{\n\t\t\tname: \"pods belong to a taskResource, Wait\",\n\t\t\tpods: []*corev1.Pod{\n\t\t\t\tst.MakePod().Name(\"pod1\").Namespace(\"ns1\").Node(\"node1\").UID(\"pod1\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr2\").Label(kusciaapisv1alpha1.TaskResourceUID, \"222\").Obj()},\n\t\t\texpected: framework.Wait,\n\t\t},\n\t\t{\n\t\t\tname: \"pods belong to a taskResource, Allow\",\n\t\t\tpods: []*corev1.Pod{\n\t\t\t\tst.MakePod().Name(\"pod1\").Namespace(\"ns1\").Node(\"node1\").UID(\"pod1\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr2\").Label(kusciaapisv1alpha1.TaskResourceUID, \"222\").Obj(),\n\t\t\t\tst.MakePod().Name(\"pod2\").Namespace(\"ns1\").Node(\"node2\").UID(\"pod2\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr2\").Label(kusciaapisv1alpha1.TaskResourceUID, \"222\").Obj()},\n\t\t\texpected: framework.Success,\n\t\t},\n\t}\n\n\ttimeout := 10 * time.Second\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\ttrMgr := core.NewTaskResourceManager(cs, snapshot, trInformer, podInformer, nsInformer, &timeout)\n\t\t\tcentralizedScheduling := &KusciaScheduling{trMgr: trMgr, frameworkHandler: f}\n\t\t\tfor _, pod := range tt.pods {\n\t\t\t\tcentralizedScheduling.Reserve(context.Background(), framework.NewCycleState(), pod, pod.Spec.NodeName)\n\t\t\t}\n\t\t\tcode, _ := centralizedScheduling.Permit(context.Background(), framework.NewCycleState(), tt.pods[0], tt.pods[0].Spec.NodeName)\n\t\t\tif code.Code() != tt.expected {\n\t\t\t\tt.Errorf(\"expected %v, got %v\", tt.expected, code.Code())\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestPostBind(t *testing.T) {\n\ttr := util.MakeTaskResource(\"ns1\", \"tr\", 2, nil)\n\ttr.UID = \"111\"\n\tctx := context.Background()\n\tcs := kusciaclientsetfake.NewSimpleClientset(tr)\n\ttrInformerFactory := kusciainformers.NewSharedInformerFactory(cs, 0)\n\ttrInformer := trInformerFactory.Kuscia().V1alpha1().TaskResources()\n\ttrInformerFactory.Start(ctx.Done())\n\n\ttrInformer.Informer().GetStore().Add(tr)\n\n\tfakeClient := clientsetfake.NewSimpleClientset()\n\tinformerFactory := informers.NewSharedInformerFactory(fakeClient, 0)\n\tpodInformer := informerFactory.Core().V1().Pods()\n\tnsInformer := informerFactory.Core().V1().Namespaces()\n\tinformerFactory.Start(ctx.Done())\n\n\texistingPods, allNodes := util.MakeNodesAndPods(map[string]string{kusciaapisv1alpha1.TaskResourceUID: \"111\"}, nil, 10, 30)\n\tsnapshot := util.NewFakeSharedLister(existingPods, allNodes)\n\t// Compose a framework handle.\n\tregisteredPlugins := []st.RegisterPluginFunc{\n\t\tst.RegisterQueueSortPlugin(queuesort.Name, queuesort.New),\n\t\tst.RegisterBindPlugin(defaultbinder.Name, defaultbinder.New),\n\t}\n\n\tf, err := st.NewFramework(registeredPlugins, \"\", ctx.Done(),\n\t\tframeworkruntime.WithClientSet(fakeClient),\n\t\tframeworkruntime.WithEventRecorder(&events.FakeRecorder{}),\n\t\tframeworkruntime.WithInformerFactory(informerFactory),\n\t\tframeworkruntime.WithSnapshotSharedLister(snapshot),\n\t)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttests := []struct {\n\t\tname            string\n\t\tpods            []*corev1.Pod\n\t\texpectedtrExist bool\n\t}{\n\t\t{\n\t\t\tname: \"pod1 does not belong to any task resource\",\n\t\t\tpods: []*corev1.Pod{\n\t\t\t\tst.MakePod().Name(\"pod1\").Namespace(\"ns1\").UID(\"pod1\").Obj(),\n\t\t\t\tst.MakePod().Name(\"pod2\").Namespace(\"ns1\").Node(\"node2\").UID(\"pod2\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr\").Label(kusciaapisv1alpha1.TaskResourceUID, \"111\").Obj()},\n\t\t\texpectedtrExist: true,\n\t\t},\n\t\t{\n\t\t\tname: \"pod1 belong to task resource tr\",\n\t\t\tpods: []*corev1.Pod{\n\t\t\t\tst.MakePod().Name(\"pod1\").Namespace(\"ns1\").Node(\"node1\").UID(\"pod1\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr\").Label(kusciaapisv1alpha1.TaskResourceUID, \"111\").Obj(),\n\t\t\t\tst.MakePod().Name(\"pod2\").Namespace(\"ns1\").Node(\"node2\").UID(\"pod2\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr\").Label(kusciaapisv1alpha1.TaskResourceUID, \"111\").Obj()},\n\t\t\texpectedtrExist: false,\n\t\t},\n\t}\n\n\ttimeout := 10 * time.Second\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tcycleState := framework.NewCycleState()\n\t\t\ttrMgr := core.NewTaskResourceManager(cs, snapshot, trInformer, podInformer, nsInformer, &timeout)\n\t\t\tcentralizedScheduling := &KusciaScheduling{trMgr: trMgr, frameworkHandler: f}\n\t\t\tfor _, pod := range tt.pods {\n\t\t\t\tcentralizedScheduling.Reserve(context.Background(), cycleState, pod, pod.Spec.NodeName)\n\t\t\t}\n\t\t\tcentralizedScheduling.PostBind(context.Background(), cycleState, tt.pods[0], tt.pods[0].Spec.NodeName)\n\t\t\ttrInfo := core.GetTaskResourceInfos(trMgr)\n\t\t\t_, exist := trInfo.Load(tr.Namespace + \"/\" + tr.Name)\n\t\t\tif exist != tt.expectedtrExist {\n\t\t\t\tt.Errorf(\"expectedtrExist %v, got %v\", tt.expectedtrExist, exist)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestPostFilter(t *testing.T) {\n\tnodeStatusMap := framework.NodeToStatusMap{\"node1\": framework.NewStatus(framework.Success, \"\")}\n\tctx := context.Background()\n\tcs := kusciaclientsetfake.NewSimpleClientset()\n\ttrInformerFactory := kusciainformers.NewSharedInformerFactory(cs, 0)\n\ttrInformer := trInformerFactory.Kuscia().V1alpha1().TaskResources()\n\ttrInformerFactory.Start(ctx.Done())\n\n\ttr := util.MakeTaskResource(\"ns1\", \"tr\", 2, nil)\n\ttr.UID = \"111\"\n\ttrInformer.Informer().GetStore().Add(tr)\n\n\tfakeClient := clientsetfake.NewSimpleClientset()\n\tinformerFactory := informers.NewSharedInformerFactory(fakeClient, 0)\n\tpodInformer := informerFactory.Core().V1().Pods()\n\tnsInformer := informerFactory.Core().V1().Namespaces()\n\tinformerFactory.Start(ctx.Done())\n\n\texistingPods, allNodes := util.MakeNodesAndPods(map[string]string{\"test\": \"a\"}, nil, 60, 30)\n\tsnapshot := util.NewFakeSharedLister(existingPods, allNodes)\n\t// Compose a framework handle.\n\tregisteredPlugins := []st.RegisterPluginFunc{\n\t\tst.RegisterQueueSortPlugin(queuesort.Name, queuesort.New),\n\t\tst.RegisterBindPlugin(defaultbinder.Name, defaultbinder.New),\n\t}\n\tf, err := st.NewFramework(registeredPlugins, \"\", ctx.Done(),\n\t\tframeworkruntime.WithClientSet(fakeClient),\n\t\tframeworkruntime.WithEventRecorder(&events.FakeRecorder{}),\n\t\tframeworkruntime.WithInformerFactory(informerFactory),\n\t\tframeworkruntime.WithSnapshotSharedLister(snapshot),\n\t)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\texistingPods, allNodes = util.MakeNodesAndPods(map[string]string{kusciaapisv1alpha1.TaskResourceUID: \"111\"},\n\t\tmap[string]string{kusciaapisv1alpha1.TaskResourceKey: \"tr\"}, 10, 30)\n\tfor _, pod := range existingPods {\n\t\tpod.Namespace = \"ns1\"\n\t}\n\tgroupPodSnapshot := util.NewFakeSharedLister(existingPods, allNodes)\n\n\ttests := []struct {\n\t\tname                 string\n\t\tpods                 []*corev1.Pod\n\t\texpectedEmptyMsg     bool\n\t\tsnapshotSharedLister framework.SharedLister\n\t}{\n\t\t{\n\t\t\tname:             \"pod does not belong to any task resource\",\n\t\t\tpods:             []*corev1.Pod{st.MakePod().Name(\"pod1\").Namespace(\"ns1\").UID(\"pod1\").Obj()},\n\t\t\texpectedEmptyMsg: false,\n\t\t},\n\t\t{\n\t\t\tname: \"enough pods assigned, do not reject all\",\n\t\t\tpods: []*corev1.Pod{\n\t\t\t\tst.MakePod().Name(\"pod1\").Namespace(\"ns1\").Node(\"node1\").UID(\"pod1\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr\").Label(kusciaapisv1alpha1.TaskResourceUID, \"111\").Obj(),\n\t\t\t\tst.MakePod().Name(\"pod2\").Namespace(\"ns1\").Node(\"node2\").UID(\"pod2\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr\").Label(kusciaapisv1alpha1.TaskResourceUID, \"111\").Obj(),\n\t\t\t\tst.MakePod().Name(\"pod3\").Namespace(\"ns1\").Node(\"node3\").UID(\"pod3\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr\").Label(kusciaapisv1alpha1.TaskResourceUID, \"111\").Obj()},\n\t\t\texpectedEmptyMsg:     true,\n\t\t\tsnapshotSharedLister: groupPodSnapshot,\n\t\t},\n\t\t{\n\t\t\tname: \"pod failed at filter phase, reject all pods\",\n\t\t\tpods: []*corev1.Pod{\n\t\t\t\tst.MakePod().Name(\"pod1\").Namespace(\"ns1\").Node(\"node1\").UID(\"pod1\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr\").Label(kusciaapisv1alpha1.TaskResourceUID, \"111\").Obj(),\n\t\t\t\tst.MakePod().Name(\"pod2\").Namespace(\"ns1\").Node(\"node2\").UID(\"pod2\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr\").Label(kusciaapisv1alpha1.TaskResourceUID, \"111\").Obj()},\n\t\t\texpectedEmptyMsg: false,\n\t\t},\n\t}\n\n\ttimeout := 10 * time.Second\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tcycleState := framework.NewCycleState()\n\t\t\tmgrSnapShot := snapshot\n\t\t\tif tt.snapshotSharedLister != nil {\n\t\t\t\tmgrSnapShot = tt.snapshotSharedLister\n\t\t\t}\n\n\t\t\ttrMgr := core.NewTaskResourceManager(cs, mgrSnapShot, trInformer, podInformer, nsInformer, &timeout)\n\t\t\tcentralizedScheduling := &KusciaScheduling{trMgr: trMgr, frameworkHandler: f}\n\t\t\tfor _, pod := range tt.pods {\n\t\t\t\tcentralizedScheduling.Reserve(context.Background(), cycleState, pod, pod.Spec.NodeName)\n\t\t\t}\n\t\t\t_, code := centralizedScheduling.PostFilter(context.Background(), cycleState, tt.pods[0], nodeStatusMap)\n\t\t\tif code.Message() == \"\" != tt.expectedEmptyMsg {\n\t\t\t\tt.Errorf(\"expectedEmptyMsg %v, got %v\", tt.expectedEmptyMsg, code.Message() == \"\")\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestPreFilter(t *testing.T) {\n\tctx := context.Background()\n\tcs := kusciaclientsetfake.NewSimpleClientset()\n\ttrInformerFactory := kusciainformers.NewSharedInformerFactory(cs, 0)\n\ttrInformer := trInformerFactory.Kuscia().V1alpha1().TaskResources()\n\ttrInformerFactory.Start(ctx.Done())\n\n\ttr := util.MakeTaskResource(\"ns1\", \"tr\", 2, nil)\n\ttr.UID = \"111\"\n\ttrInformer.Informer().GetStore().Add(tr)\n\n\tfakeClient := clientsetfake.NewSimpleClientset()\n\tinformerFactory := informers.NewSharedInformerFactory(fakeClient, 0)\n\tpodInformer := informerFactory.Core().V1().Pods()\n\tnsInformer := informerFactory.Core().V1().Namespaces()\n\tinformerFactory.Start(ctx.Done())\n\n\tns1 := &corev1.Namespace{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName: \"ns1\",\n\t\t},\n\t}\n\tnsInformer.Informer().GetStore().Add(ns1)\n\n\texistingPods, allNodes := util.MakeNodesAndPods(map[string]string{\"test\": \"a\"}, nil, 60, 30)\n\tsnapshot := util.NewFakeSharedLister(existingPods, allNodes)\n\t// Compose a framework handle.\n\tregisteredPlugins := []st.RegisterPluginFunc{\n\t\tst.RegisterQueueSortPlugin(queuesort.Name, queuesort.New),\n\t\tst.RegisterBindPlugin(defaultbinder.Name, defaultbinder.New),\n\t}\n\tf, err := st.NewFramework(registeredPlugins, \"\", ctx.Done(),\n\t\tframeworkruntime.WithClientSet(fakeClient),\n\t\tframeworkruntime.WithEventRecorder(&events.FakeRecorder{}),\n\t\tframeworkruntime.WithInformerFactory(informerFactory),\n\t\tframeworkruntime.WithSnapshotSharedLister(snapshot),\n\t)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttests := []struct {\n\t\tname             string\n\t\tpods             []*corev1.Pod\n\t\tcleantrInformer  bool\n\t\texpectedEmptyMsg bool\n\t}{\n\t\t{\n\t\t\tname:             \"pod does not belong to any task resource\",\n\t\t\tpods:             []*corev1.Pod{st.MakePod().Name(\"pod1\").Namespace(\"ns1\").UID(\"pod1\").Obj()},\n\t\t\tcleantrInformer:  true,\n\t\t\texpectedEmptyMsg: true,\n\t\t},\n\t\t{\n\t\t\tname: \"pod belong to task resource tr, but tr does node exist\",\n\t\t\tpods: []*corev1.Pod{\n\t\t\t\tst.MakePod().Name(\"pod1\").Namespace(\"ns1\").Node(\"node1\").UID(\"pod1\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr\").Label(kusciaapisv1alpha1.TaskResourceUID, \"111\").Obj()},\n\t\t\tcleantrInformer:  true,\n\t\t\texpectedEmptyMsg: false,\n\t\t},\n\t\t{\n\t\t\tname: \"pod belong to task resource tr, but the member less than MinReservedPods\",\n\t\t\tpods: []*corev1.Pod{\n\t\t\t\tst.MakePod().Name(\"pod1\").Namespace(\"ns1\").Node(\"node1\").UID(\"pod1\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr\").Label(kusciaapisv1alpha1.TaskResourceUID, \"111\").Obj()},\n\t\t\texpectedEmptyMsg: false,\n\t\t},\n\t}\n\n\ttimeout := 10 * time.Second\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tcycleState := framework.NewCycleState()\n\t\t\tif tt.cleantrInformer {\n\t\t\t\ttrInformer.Informer().GetStore().Delete(tr)\n\t\t\t\tdefer trInformer.Informer().GetStore().Add(tr)\n\t\t\t}\n\n\t\t\ttrMgr := core.NewTaskResourceManager(cs, snapshot, trInformer, podInformer, nsInformer, &timeout)\n\t\t\tcentralizedScheduling := &KusciaScheduling{trMgr: trMgr, frameworkHandler: f}\n\n\t\t\t_, code := centralizedScheduling.PreFilter(context.Background(), cycleState, tt.pods[0])\n\t\t\tif code.Message() == \"\" != tt.expectedEmptyMsg {\n\t\t\t\tt.Errorf(\"expectedEmptyMsg %v, got %v\", tt.expectedEmptyMsg, code.Message() == \"\")\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestUnreserve(t *testing.T) {\n\ttr := util.MakeTaskResource(\"ns1\", \"tr\", 2, nil)\n\ttr.UID = \"111\"\n\tctx := context.Background()\n\tcs := kusciaclientsetfake.NewSimpleClientset(tr)\n\ttrInformerFactory := kusciainformers.NewSharedInformerFactory(cs, 0)\n\ttrInformer := trInformerFactory.Kuscia().V1alpha1().TaskResources()\n\ttrInformerFactory.Start(ctx.Done())\n\n\ttrInformer.Informer().GetStore().Add(tr)\n\n\tfakeClient := clientsetfake.NewSimpleClientset()\n\tinformerFactory := informers.NewSharedInformerFactory(fakeClient, 0)\n\tpodInformer := informerFactory.Core().V1().Pods()\n\tnsInformer := informerFactory.Core().V1().Namespaces()\n\tinformerFactory.Start(ctx.Done())\n\n\texistingPods, allNodes := util.MakeNodesAndPods(map[string]string{kusciaapisv1alpha1.TaskResourceUID: \"111\"}, nil, 10, 30)\n\tsnapshot := util.NewFakeSharedLister(existingPods, allNodes)\n\t// Compose a framework handle.\n\tregisteredPlugins := []st.RegisterPluginFunc{\n\t\tst.RegisterQueueSortPlugin(queuesort.Name, queuesort.New),\n\t\tst.RegisterBindPlugin(defaultbinder.Name, defaultbinder.New),\n\t}\n\n\tf, err := st.NewFramework(registeredPlugins, \"\", ctx.Done(),\n\t\tframeworkruntime.WithClientSet(fakeClient),\n\t\tframeworkruntime.WithEventRecorder(&events.FakeRecorder{}),\n\t\tframeworkruntime.WithInformerFactory(informerFactory),\n\t\tframeworkruntime.WithSnapshotSharedLister(snapshot),\n\t)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttests := []struct {\n\t\tname            string\n\t\tpods            []*corev1.Pod\n\t\texpectedtrExist bool\n\t}{\n\t\t{\n\t\t\tname: \"pod1 does not belong to any task resource\",\n\t\t\tpods: []*corev1.Pod{\n\t\t\t\tst.MakePod().Name(\"pod1\").Namespace(\"ns1\").UID(\"pod1\").Obj(),\n\t\t\t\tst.MakePod().Name(\"pod2\").Namespace(\"ns1\").Node(\"node2\").UID(\"pod2\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr\").Label(kusciaapisv1alpha1.TaskResourceUID, \"111\").Obj()},\n\t\t\texpectedtrExist: true,\n\t\t},\n\t\t{\n\t\t\tname: \"pod1 belong to task resource tr\",\n\t\t\tpods: []*corev1.Pod{\n\t\t\t\tst.MakePod().Name(\"pod1\").Namespace(\"ns1\").Node(\"node1\").UID(\"pod1\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr\").Label(kusciaapisv1alpha1.TaskResourceUID, \"111\").Obj(),\n\t\t\t\tst.MakePod().Name(\"pod2\").Namespace(\"ns1\").Node(\"node2\").UID(\"pod2\").\n\t\t\t\t\tAnnotation(kusciaapisv1alpha1.TaskResourceKey, \"tr\").Label(kusciaapisv1alpha1.TaskResourceUID, \"111\").Obj()},\n\t\t\texpectedtrExist: false,\n\t\t},\n\t}\n\n\ttimeout := 10 * time.Second\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tcycleState := framework.NewCycleState()\n\t\t\ttrMgr := core.NewTaskResourceManager(cs, snapshot, trInformer, podInformer, nsInformer, &timeout)\n\t\t\tcentralizedScheduling := &KusciaScheduling{trMgr: trMgr, frameworkHandler: f}\n\t\t\tfor _, pod := range tt.pods {\n\t\t\t\tcentralizedScheduling.Reserve(context.Background(), cycleState, pod, pod.Spec.NodeName)\n\t\t\t}\n\t\t\tcentralizedScheduling.Unreserve(context.Background(), cycleState, tt.pods[0], tt.pods[0].Spec.NodeName)\n\t\t\ttrInfo := core.GetTaskResourceInfos(trMgr)\n\t\t\t_, exist := trInfo.Load(tr.Namespace + \"/\" + tr.Name)\n\t\t\tif exist != tt.expectedtrExist {\n\t\t\t\tt.Errorf(\"expectedtrExist %v, got %v\", tt.expectedtrExist, exist)\n\t\t\t}\n\t\t})\n\t}\n}"
