[meta]
task = "update"
scenario = "update_test"

[lang_info]
lang = "Python"
python_version = "3.11"
python_cfg_file = "requirements.txt"

[repo_info]
repository = "google/langfun"
sha = "11d646e44baaea09bb792c3428a8f71ba742363b"

[run_info]
docker_image = "python:3.11"
volumes_to_mount = [ "{proj_path}:/app", "{host}/.m2:/.m2", "{host}/.cache/pip:/.pip_cache", "{host}/.cache/go-build:/.go_cache", "{proj_path}/_HOME_/go:/go",]
docker_wrap = "sudo docker run --rm -w /app -e PATH=_HOME_/.local/bin:$PATH -e PYTHONUSERBASE=_HOME_/.local/ -v {proj_path}:/app -v {host}/.m2:/.m2 -v {host}/.cache/pip:/.pip_cache -v {host}/.cache/go-build:/.go_cache -v {proj_path}/_HOME_/go:/go {img} sh -c '{cmd}'"
env = [ "PATH=_HOME_/.local/bin:$PATH", "PYTHONUSERBASE=_HOME_/.local/",]
prebuild_command = "pip install -r requirements.txt && (pip install git+https://github.com/Klema17/mutpy.git && pip install coverage pytest pytest_cov covdefaults Cython mock ddt pytest_mock testfixtures)"
test_run_command = "coverage run --include=langfun/core/eval/v2/evaluation.py -m pytest -q --junit-xml=test_output.xml langfun/core/eval/v2/evaluation_test.py && coverage xml -o coverage.xml --fail-under=0"
mutation_run_command = "mut.py --target langfun.core.eval.v2.evaluation --unit-test langfun.core.eval.v2.evaluation_test --runner pytest --report mutation_report.yaml"
mutation_run_command_fallback = "mut.py --target langfun/core/eval/v2/evaluation.py --unit-test langfun/core/eval/v2/evaluation_test.py --runner pytest --report mutation_report.yaml"
coverage_report_path = "coverage.xml"
coverage_report_type = "cobertura"
mutation_report_path = "mutation_report.yaml"
mutation_report_type = "mutpy"

[coverage]
coverage = 83.0
original_coverage = 94.0
mutation_kill_rate = nan
original_mutation_kill_rate = 99.0
covered_lines = [ 15, 16, 17, 18, 19, 21, 22, 23, 25, 26, 27, 28, 30, 33, 47, 52, 57, 62, 64, 65, 66, 67, 68, 74, 75, 77, 79, 80, 82, 84, 85, 86, 87, 88, 89, 95, 96, 98, 100, 102, 103, 105, 106, 108, 110, 111, 115, 116, 117, 118, 119, 121, 122, 128, 129, 147, 161, 162, 163, 165, 166, 168, 169, 170, 171, 172, 191, 192, 193, 194, 195, 199, 200, 202, 203, 204, 205, 207, 208, 210, 216, 224, 225, 226, 227, 232, 233, 234, 244, 254, 263, 264, 266, 268, 287, 289, 290, 292, 293, 294, 300, 302, 303, 304, 305, 308, 314, 315, 317, 319, 321, 323, 325, 327, 329, 331, 333, 336, 342, 345, 350, 351, 352, 353, 356, 357, 359, 380, 386, 387, 414, 419, 420, 452, 454, 462, 464, 484, 486, 506, 510, 528, 530, 531, 532, 547, 548, 564, 581, 582, 586, 587, 689, 692, 694, 699, 704, 709, 710, 711, 712, 716, 739, 740, 744, 745, 749, 751, 753, 759, 761, 763, 765, 767,]
missed_lines = [ 83, 174, 177, 178, 179, 184, 185, 230, 235, 236, 237, 238, 277, 278, 284, 285, 346, 361, 368, 369, 370, 371, 372, 373, 726, 727, 728, 733, 734, 735, 736, 737, 742, 747, 755,]

[input_info]
test_file_path = "langfun/core/eval/v2/evaluation_test.py"
focal_file_path = "langfun/core/eval/v2/evaluation.py"
test_file_url = "https://github.com/google/langfun/blob/11d646e44baaea09bb792c3428a8f71ba742363b/langfun/core/eval/v2/evaluation_test.py"
focal_file_url = "https://github.com/google/langfun/blob/11d646e44baaea09bb792c3428a8f71ba742363b/langfun/core/eval/v2/evaluation.py"
first_commit_date = "2024-11-12"
last_commit_date = "2025-03-21"
test_file_content = "# Copyright 2024 The Langfun Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport tempfile\nimport unittest\n\nfrom langfun.core.eval.v2 import eval_test_helper\nfrom langfun.core.eval.v2 import evaluation as evaluation_lib\nfrom langfun.core.eval.v2 import example as example_lib\nfrom langfun.core.eval.v2 import experiment as experiment_lib\n\nimport pyglove as pg\n\nExample = example_lib.Example\nEvaluation = evaluation_lib.Evaluation\nRunId = experiment_lib.RunId\nRun = experiment_lib.Run\n\n\nclass EvaluationTest(unittest.TestCase):\n\n  def test_hyper_evaluation(self):\n    exp = eval_test_helper.TestEvaluation(\n        lm=eval_test_helper.TestLLM(offset=pg.oneof(range(3)))\n    )\n    self.assertFalse(exp.is_leaf)\n    self.assertTrue(\n        pg.eq(\n            exp.children,\n            [\n                eval_test_helper.TestEvaluation(\n                    lm=eval_test_helper.TestLLM(offset=0)\n                ),\n                eval_test_helper.TestEvaluation(\n                    lm=eval_test_helper.TestLLM(offset=1)\n                ),\n                eval_test_helper.TestEvaluation(\n                    lm=eval_test_helper.TestLLM(offset=2)\n                ),\n            ]\n        )\n    )\n    self.assertEqual(exp.children[0].num_examples, 10)\n    self.assertEqual(\n        [c.is_leaf for c in exp.children],\n        [True] * len(exp.children)\n    )\n    self.assertEqual(\n        [r.resource_ids() for r in exp.leaf_nodes],\n        [set(['test_llm:0']), set(['test_llm:1']), set(['test_llm:2'])]\n    )\n\n  def test_input(self):\n    exp = eval_test_helper.TestEvaluation()\n    self.assertEqual(exp.num_examples, 10)\n    exp = eval_test_helper.TestEvaluation(\n        inputs=eval_test_helper.test_inputs(None)\n    )\n    self.assertEqual(exp.num_examples, 20)\n    @pg.functor\n    def my_inputs():\n      yield pg.Dict(x=1, y=2)\n      yield pg.Dict(x=3, y=4)\n    exp = eval_test_helper.TestEvaluation(inputs=my_inputs())\n    self.assertEqual(exp.num_examples, 2)\n\n  def test_evaluate(self):\n    exp = eval_test_helper.TestEvaluation()\n    example = exp.evaluate(Example(id=3))\n    self.assertIs(exp.state.get(3), example)\n    self.assertTrue(example.newly_processed)\n    self.assertEqual(example.input, pg.Dict(x=2, y=4, groundtruth=6))\n    self.assertEqual(example.output, 6)\n    self.assertIsNone(example.error)\n    self.assertEqual(example.metadata, {})\n    self.assertEqual(example.metric_metadata, dict(match=True))\n    self.assertIsNotNone(example.usage_summary)\n    self.assertGreater(example.usage_summary.total.total_tokens, 0)\n    self.assertEqual(example.usage_summary.total.num_requests, 1)\n    self.assertIsNotNone(example.execution_status)\n    self.assertIsNotNone(example.start_time)\n    self.assertIsNotNone(example.end_time)\n\n    exp = eval_test_helper.TestEvaluation(lm=eval_test_helper.TestLLM(offset=1))\n    example = exp.evaluate(3)\n    self.assertTrue(example.newly_processed)\n    self.assertEqual(example.input, pg.Dict(x=2, y=4, groundtruth=6))\n    self.assertEqual(example.output, 7)\n    self.assertIsNone(example.error)\n    self.assertEqual(example.metadata, {})\n    self.assertEqual(example.metric_metadata, dict(mismatch=True))\n\n    with self.assertRaisesRegex(ValueError, 'x should not be 5'):\n      _ = exp.evaluate(6, raise_if_has_error=True)\n    example = exp.evaluate(6)\n    self.assertTrue(example.newly_processed)\n    self.assertEqual(example.input, pg.Dict(x=5, y=25, groundtruth=30))\n    self.assertEqual(pg.MISSING_VALUE, example.output)\n    self.assertEqual(example.error.tag, 'ValueError')\n    self.assertEqual(example.metadata, {})\n    self.assertEqual(example.metric_metadata, dict(error='ValueError'))\n\n  def test_evaluate_with_state(self):\n    eval_dir = os.path.join(tempfile.gettempdir(), 'test_eval')\n    pg.io.mkdirs(eval_dir, exist_ok=True)\n    state_file = os.path.join(eval_dir, 'state.jsonl')\n    with pg.io.open_sequence(state_file, 'w') as f:\n      exp = eval_test_helper.TestEvaluation()\n      example = exp.evaluate(3)\n      self.assertTrue(example.newly_processed)\n      self.assertEqual(example.input, pg.Dict(x=2, y=4, groundtruth=6))\n      self.assertEqual(example.output, 6)\n      self.assertEqual(len(exp._state.evaluated_examples), 1)\n      f.add(pg.to_json_str(example))\n\n    exp.reset()\n    self.assertEqual(len(exp._state.evaluated_examples), 0)\n    exp.load_state(state_file)\n    self.assertEqual(len(exp._state.evaluated_examples), 1)\n    example = exp.evaluate(3)\n    self.assertFalse(example.newly_processed)\n    self.assertEqual(example.input, pg.Dict(x=2, y=4, groundtruth=6))\n    self.assertEqual(example.output, 6)\n    self.assertGreater(example.usage_summary.total.total_tokens, 0)\n    self.assertGreater(example.usage_summary.cached.total.total_tokens, 0)\n    self.assertEqual(example.usage_summary.cached.total.num_requests, 1)\n    self.assertEqual(example.usage_summary.uncached.total.total_tokens, 0)\n    self.assertEqual(example.usage_summary.uncached.total.num_requests, 0)\n\n  def test_html_view(self):\n    exp = eval_test_helper.TestEvaluation()\n    exp.debug('debug message')\n    exp.info('info message')\n    exp.warning('warning message', x=1)\n    exp.error('error message', x=1)\n    exp.fatal('fatal message')\n\n    self.assertIn(\n        exp.id,\n        exp.to_html(extra_flags=dict(card_view=True, current_run=None)).content\n    )\n    self.assertIn(\n        exp.id,\n        exp.to_html(\n            extra_flags=dict(\n                card_view=False,\n                current_run=Run(\n                    root_dir='/tmp/test_run',\n                    id=RunId.from_id('20241031_1'),\n                    experiment=pg.Ref(exp),\n                )\n            )\n        ).content\n    )\n\n\nif __name__ == '__main__':\n  unittest.main()"
